1
00:00:00,000 --> 00:00:04,500
So, welcome to the lecture about Gradient Descent Updater Strategies.

2
00:00:04,500 --> 00:00:07,095
This is a very important lecture because choosing

3
00:00:07,095 --> 00:00:12,370
the correct Gradient Descent Updater Strategy might heavily influence your learning.

4
00:00:12,370 --> 00:00:15,124
So remember, in Gradient Descent,

5
00:00:15,124 --> 00:00:18,075
we are testing one random value because

6
00:00:18,075 --> 00:00:23,035
that random value is generated by the random initialization of the weights.

7
00:00:23,035 --> 00:00:27,255
And from there, you have to find the next step.

8
00:00:27,255 --> 00:00:28,935
So next step, you find, of course,

9
00:00:28,935 --> 00:00:31,650
by computing the first derivative of

10
00:00:31,650 --> 00:00:35,650
the cost function and you go in direction of the steepest descent.

11
00:00:35,650 --> 00:00:38,165
But there are some catches,

12
00:00:38,165 --> 00:00:41,130
so let's start with Gradient Descent as it is.

13
00:00:41,130 --> 00:00:42,990
So in Gradient Descent,

14
00:00:42,990 --> 00:00:44,280
you update the weights,

15
00:00:44,280 --> 00:00:47,546
theta, by subtracting a value from it.

16
00:00:47,546 --> 00:00:50,525
So, the value which we are subtraction is this one.

17
00:00:50,525 --> 00:00:53,855
So, let's have a look how this is computed.

18
00:00:53,855 --> 00:00:56,695
Again, it starts with the cost function J.

19
00:00:56,695 --> 00:00:58,080
The cost function J is,

20
00:00:58,080 --> 00:00:59,545
of course, dependent on theta,

21
00:00:59,545 --> 00:01:03,495
the weights, but also on the training data.

22
00:01:03,495 --> 00:01:05,107
So the training data,

23
00:01:05,107 --> 00:01:08,115
I have exemplified here using Matrix X,

24
00:01:08,115 --> 00:01:09,920
and the Matrix Y.

25
00:01:09,920 --> 00:01:13,170
So, this is basically the complete data set.

26
00:01:13,170 --> 00:01:17,935
And once this cost function is computed under complete data set,

27
00:01:17,935 --> 00:01:19,770
we take the first derivative,

28
00:01:19,770 --> 00:01:21,555
we get the gradient,

29
00:01:21,555 --> 00:01:24,360
then we multiply by the learning rate.

30
00:01:24,360 --> 00:01:26,225
So, the update would be smaller,

31
00:01:26,225 --> 00:01:30,610
and then we subtract from the actual value of theta_t,

32
00:01:30,610 --> 00:01:33,545
and that gives us theta_t plus one.

33
00:01:33,545 --> 00:01:36,540
So, that's how gradient descent works.

34
00:01:36,540 --> 00:01:37,735
So, there's a variation.

35
00:01:37,735 --> 00:01:40,275
This is called Stochastic Gradient Descent,

36
00:01:40,275 --> 00:01:44,370
and the only difference is that you are not computing the gradient under

37
00:01:44,370 --> 00:01:48,845
complete matrix X and the complete matrix Y.

38
00:01:48,845 --> 00:01:52,555
You just take one training example. This is X_i and Y_i.

39
00:01:52,555 --> 00:01:56,025
So once you have computed the gradient,

40
00:01:56,025 --> 00:01:59,295
you update already the parameters theta.

41
00:01:59,295 --> 00:02:03,380
So, a variation of this is the so-called Mini Batch Gradient Descent,

42
00:02:03,380 --> 00:02:05,350
so that's somehow in between.

43
00:02:05,350 --> 00:02:11,105
You don't use the complete data set and just don't use a single instance of your data,

44
00:02:11,105 --> 00:02:13,365
but just use a batch.

45
00:02:13,365 --> 00:02:16,390
So the batch size, you have to define.

46
00:02:16,390 --> 00:02:20,560
Usually, you take values between 32 and 1024 or so,

47
00:02:20,560 --> 00:02:24,395
and then you compute the gradient for that particular batch.

48
00:02:24,395 --> 00:02:27,040
And once this gradient has been computed,

49
00:02:27,040 --> 00:02:29,205
you update the parameter matrix theta.

50
00:02:29,205 --> 00:02:32,360
So now, another way of doing this is called momentum.

51
00:02:32,360 --> 00:02:37,255
So momentum, the idea is that we take also an update of

52
00:02:37,255 --> 00:02:42,800
a past timestep into consideration for computing the update of the current timestep.

53
00:02:42,800 --> 00:02:45,085
So now, we have here a variable called nu.

54
00:02:45,085 --> 00:02:49,840
So, nu is computed from nu_t minus one.

55
00:02:49,840 --> 00:02:53,760
And usually, we take a gamma of 0-9.

56
00:02:53,760 --> 00:02:57,310
And from this, it basically subtract the actual gradient.

57
00:02:57,310 --> 00:02:58,915
Once you've computed nu_t,

58
00:02:58,915 --> 00:03:04,895
we subtract nu_t from theta_t and we get the updated theta_t plus one.

59
00:03:04,895 --> 00:03:07,205
So, there's a variation of momentum.

60
00:03:07,205 --> 00:03:10,015
This is called Nesterov Accelerated Gradient,

61
00:03:10,015 --> 00:03:13,600
and the only addition here is that in the cost function,

62
00:03:13,600 --> 00:03:17,110
we subtract this term here already from theta_t.

63
00:03:17,110 --> 00:03:22,755
So the usual way, this is like a ball rolling down the hill and it's a smart ball.

64
00:03:22,755 --> 00:03:25,630
So whenever the slope starts to increase,

65
00:03:25,630 --> 00:03:28,900
the ball stops accelerating and breaks a bit.

66
00:03:28,900 --> 00:03:32,170
So Adagrad, which stands for a Adaptive Gradient,

67
00:03:32,170 --> 00:03:37,160
tries to change the learning rate over time and not only for a complete batch.

68
00:03:37,160 --> 00:03:41,440
It tries to change the learner rate for each individual example.

69
00:03:41,440 --> 00:03:44,875
So you see here, this formula is dependent on i.

70
00:03:44,875 --> 00:03:46,980
So, i stands for extra training example.

71
00:03:46,980 --> 00:03:50,590
You'll see here that the learning rate eta

72
00:03:50,590 --> 00:03:54,410
is modified and it's modified by this term here.

73
00:03:54,410 --> 00:03:58,540
So, G_t, ii is the matrix which contains

74
00:03:58,540 --> 00:04:03,050
information on the past gradients per training example.

75
00:04:03,050 --> 00:04:04,915
And taking this into account,

76
00:04:04,915 --> 00:04:08,285
it just reduces the learning rate.

77
00:04:08,285 --> 00:04:10,845
And note that there is little term, e,

78
00:04:10,845 --> 00:04:14,415
so that we avoid division by zero.

79
00:04:14,415 --> 00:04:17,480
This whole thing cannot only be computed per

80
00:04:17,480 --> 00:04:21,565
example but also using matrix-matrix multiplication,

81
00:04:21,565 --> 00:04:23,405
and therefore, we can omit the i,

82
00:04:23,405 --> 00:04:26,590
because G_t is a diagonal matrix.

83
00:04:26,590 --> 00:04:28,560
So, don't worry if you don't understand this,

84
00:04:28,560 --> 00:04:31,965
so maybe you have to revisit the linear algebra basics,

85
00:04:31,965 --> 00:04:36,325
but anyway, it's just the way to compute the whole math in just one go.

86
00:04:36,325 --> 00:04:39,770
So, eta data is only a variation of Adagrad,

87
00:04:39,770 --> 00:04:44,075
and the difference is that it doesn't use a matrix G,

88
00:04:44,075 --> 00:04:48,210
but it continuously computes the mean of the historic gradients.

89
00:04:48,210 --> 00:04:52,045
So there are many, many variations of gradient descent updates,

90
00:04:52,045 --> 00:04:56,695
and I recommend you to have a look at Sebastian's blog later,

91
00:04:56,695 --> 00:05:00,220
where everything is described really nicely.

92
00:05:00,220 --> 00:05:02,490
But the key take home point here is that

93
00:05:02,490 --> 00:05:07,500
the Gradient Descent Updater strategy is very important note you have to tune,

94
00:05:07,500 --> 00:05:12,595
and as usual, tuning a neural network is considered as black magic or trial and error.

95
00:05:12,595 --> 00:05:15,875
So, you just have to try a couple of those.

96
00:05:15,875 --> 00:05:19,890
So, let's actually have a look at Sebastian's blog.

97
00:05:19,890 --> 00:05:22,175
So, Sebastian is this funny guy here.

98
00:05:22,175 --> 00:05:25,170
And you see here,

99
00:05:25,170 --> 00:05:28,220
those are the trajectories of different.

100
00:05:28,220 --> 00:05:34,135
learning course, using different gradient descent updater strategies.

101
00:05:34,135 --> 00:05:39,915
So, it's pretty interesting because it's a very important hyper parameter you can tune.

102
00:05:39,915 --> 00:05:43,193
So, we have covered most of those.

103
00:05:43,193 --> 00:05:51,716
But what I wanted to show you is two daily figures here.

104
00:05:51,716 --> 00:05:56,270
So, those two are really interesting.

105
00:05:56,270 --> 00:05:58,550
So you see here, different trajectories in

106
00:05:58,550 --> 00:06:02,893
optimization space using different gradient descent updaters.

107
00:06:02,893 --> 00:06:09,110
So, the red one here is stochastic gradient descent and you see here in both problems,

108
00:06:09,110 --> 00:06:11,375
it performs really poor.

109
00:06:11,375 --> 00:06:13,880
And it even gets stuck.

110
00:06:13,880 --> 00:06:15,745
So maybe in the first image,

111
00:06:15,745 --> 00:06:21,320
it won't get stuck but it won't converge for ages and in the second,

112
00:06:21,320 --> 00:06:23,880
this is a settle point. It even gets stuck.

113
00:06:23,880 --> 00:06:26,230
And you can see here that for example,

114
00:06:26,230 --> 00:06:30,890
AdaDelta is performing best among all of those, Okay?

115
00:06:30,890 --> 00:06:32,720
I think that's enough for now.

116
00:06:32,720 --> 00:06:34,760
I hope that you now understood that this is

117
00:06:34,760 --> 00:06:39,040
an important hyper parameter that you can tune, and that's basically it.