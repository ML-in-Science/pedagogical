Let's get started with recurrent neural networks. So, in the last lecture, we've learned that a single layer feed forward neural networks can represent any mathematical function. But we've briefly explained that training such a neural network is impossible. At least the tools and methods we have at the moment. If you give me a quantum computer and a bit of time, I might come up with a better solution. So, since training is such a difficult task, people came up with different types of layers and topologies and recurrent neural networks is one of them. So while feed forward neural networks are good at learning the function, they fail off when it comes to sequence and time series data, like we have for example in IoT sense of data. Processing sequences and time series require some sort of memory since dynamic temporal behavior is also adding information to the whole picture. So, by introducing loop back connections between the neurons, such a recurrent neural network can remember past events. Note that any recurrent neural network can be unfolded through time and then basically forms a feed forward neural network. So again, this whole exercise is only to make training more efficient. There exists an abundance of recurrent neural network types, recursive, hopfield, fully recurrent, Elman network, Jordan network, echo state, neural history compressor and finally an LSTM. We will only concentrate on the LSTM in this course. It stands for long short term memory. Those are so powerful and essential that we dedicate the next lecture entirely to LSTMs.