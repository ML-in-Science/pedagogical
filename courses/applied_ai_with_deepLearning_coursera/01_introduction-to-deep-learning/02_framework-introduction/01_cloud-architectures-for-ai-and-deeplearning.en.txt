In this module, will be a cover AI and Deep Learning Architectures. When it comes to machine learning, there's nothing you can do than getting as much good quality data as possible. Note that I'm talking about good data. This is especially true for deep learning. So it's always good to collect as much data as possible. And one important cloud based solution to store that data is ObjectStar. So in ObjectStar, you have unlimited capacity, redundancy, fail-over, and automated backup, and the best I/O performance. Therefore, during the course, you make sure you understand how to directly train a neural network from data residing in an ObjectStar. Another aspect is Real-Time Data Processing. Often, data loses value within a couple of seconds. So think about stock market data, cryptocurrency market data, healthcare, and IoT data. Therefore, we make sure that you also learn how to apply those models on a Real-Time Data Stream. Finally, those models make heavy use of CPUs, and GPUs. Therefore, we will explain how to scale Deep Learning Models on GPU and CPU clusters. But that's only one part of the story. Before we can train a Deep Learning Neural Network, you actually have to create it. In this course, we will use Jupyter Notebooks for all the coding and we will use a cloud-based version of it. This is called Data Science Experience, so no need for installing anything on your machine. No deep learning, without a deep learning framework. And currently it's pretty hard to choose an optimal framework for given task. There exists high-level frameworks that you simply define your layers, or low-level frameworks but you have to implement everything on a linear algebra level. Unless explicitly needed, we don't recommend using of any of the low-level frameworks. Therefore, we will use Keras as a high-level framework whenever possible. You can implement a deep learning neural network with Keras within minutes. The good thing is that Keras uses low-level deep learning frameworks for execution. So per deploy Keras uses TensorFlow as execution engine. In addition, CNTK and Theano are supported. But by the way, Theano has been deprecated. So that's only TensorFlow and CNTK left. In addition, Keras can export models. Those models can be imparted by other frameworks, like DeepLearning4J and Apache SystemML, which we are using in this course as well. Keras exports a no open standard, but a lot of framework's actually support those. Been talking about open neural network exchange formats, we have to talk about ONNX as well. ONNX tries to establish an open standard, and currently Caffe too. The Cognitive Toolkit, and MXnet are supported. But none of those frameworks can be considered as high-level frameworks. Therefore, in this cost, we stick to Keras. Finally, as execution environment for parallel execution, we will use Apache Spark. Apache Spark is the de facto standard for large scale data processing, academic and enterprise environments, and it's also available as a service from different cloud providers. Data preprocessing is more straightforward in Apaches Spark than in TensorFlow for example. Finally, we will use what's in machine learning from other deployment on GPU clusters. In this case, we'll using a commercial offering from IBM, since that doesn't exist any open-source alternative. This nicely illustrates how automatic and scalable deep learning model deployment has to look like. And again, that always exists to free tier in the IBM Cloud, so you can use what's a machine learning for free as well. So this leads us to the final architecture. We will use Jupyter Notebooks on top of Data Science Experience for coding. We will use the Keras framework in conjunction with TensorFlow for development and testing of the models. If you are happy with the model, it will export it, and run it either on DeepLearning4J or Apache SystemML. Both frameworks are capable of running neural networks and parallel on top of Apache Spark. And in both frameworks, to convert on petabytes of data residing in ObjectStar. All work on live streams of data in the gigbits second range. We will exemplify this, by subscribing to topics on an MQTT message broker provided by the IBM buxon IoT platform. Instead of Apache Spark, we will also see how to use Watson machine learning for model deployment. This gives us nearly unlimited scalability and throughput.