1
00:00:01,830 --> 00:00:05,780
A drawback of direct sole method is that

2
00:00:05,780 --> 00:00:09,875
the inputs to the sole building function should be small enough.

3
00:00:09,875 --> 00:00:12,725
That is, both X transpose X,

4
00:00:12,725 --> 00:00:18,993
and X transpose Y should fit in the driver here.

5
00:00:18,993 --> 00:00:20,898
Hence, this method does not work for larger dataset.

6
00:00:20,898 --> 00:00:23,663
To get around this problem,

7
00:00:23,663 --> 00:00:26,341
we'll implement two iterative algorithm,

8
00:00:26,341 --> 00:00:34,441
that's key to a larger data set: Batch Gradient Descent and Conjugate Gradient Method.

9
00:00:34,441 --> 00:00:38,422
Batch Gradient Descent is an extremely simple algorithm.

10
00:00:38,422 --> 00:00:42,740
Assume that you are somewhere on the hill,

11
00:00:42,740 --> 00:00:47,220
and you want to reach to the bottom of the hill.

12
00:00:47,220 --> 00:00:51,057
Unfortunately, you don't have a GPS but

13
00:00:51,057 --> 00:00:55,135
you have a device that tells you slope at a given point.

14
00:00:55,135 --> 00:00:58,335
For higher dimensional surfaces,

15
00:00:58,335 --> 00:01:00,450
the slope is called as a gradient.

16
00:01:00,450 --> 00:01:05,525
In other word, gradient is just another name for derivative of

17
00:01:05,525 --> 00:01:08,290
a function and is a vector that

18
00:01:08,290 --> 00:01:13,717
points in the direction of the greatest increase of the function.

19
00:01:13,717 --> 00:01:17,878
Therefore, to reach the bottom of the hill,

20
00:01:17,878 --> 00:01:23,873
all one has to do is compute the gradient at the current location and take

21
00:01:23,873 --> 00:01:26,854
a step in the opposite direction of the gradient.

22
00:01:26,854 --> 00:01:31,564
Then, compute the gradient at that location and take a step in

23
00:01:31,564 --> 00:01:33,375
the opposite direction of that gradient,

24
00:01:33,375 --> 00:01:34,910
and so on and so forth.

25
00:01:34,910 --> 00:01:36,769
It will go out quickly,

26
00:01:36,769 --> 00:01:39,120
it is represented here.

27
00:01:39,120 --> 00:01:45,664
Step one, start with an initial point W. Continue until not converged.

28
00:01:45,664 --> 00:01:49,060
Step two, compute gradient DW.

29
00:01:49,060 --> 00:01:52,665
Step three, compute step size alpha.

30
00:01:52,665 --> 00:01:57,625
Step four, compute new W by subtracting

31
00:01:57,625 --> 00:02:03,280
alpha times DW from old W. As shown in the figure,

32
00:02:03,280 --> 00:02:07,825
if the surface has multiple local minimum,

33
00:02:07,825 --> 00:02:13,785
different initial point can lead as two different minimum.

34
00:02:16,270 --> 00:02:23,145
As discussed earlier, the gradient for the least cost function is

35
00:02:23,145 --> 00:02:29,130
X transpose X times W minus X transpose Y.

36
00:02:29,130 --> 00:02:30,705
To compute step size,

37
00:02:30,705 --> 00:02:34,595
one has to perform a line search along the gradient.

38
00:02:34,595 --> 00:02:40,675
I will leave the derivation of this expression as homework.

39
00:02:40,675 --> 00:02:45,960
The DML script for the batch gradient descent is given here.

40
00:02:46,590 --> 00:02:53,020
We initialize the starting point W to zero,

41
00:02:53,020 --> 00:02:57,667
then we take exactly 100 steps.

42
00:02:57,667 --> 00:03:01,618
In each step, we compute the gradient DW,

43
00:03:01,618 --> 00:03:05,650
and the step size alpha using the above formula.

44
00:03:05,650 --> 00:03:11,299
Then, we compute the new W by subtracting alpha times

45
00:03:11,299 --> 00:03:18,580
DW from old W. Again, as in previous example, we plot the line from

46
00:03:31,327 --> 00:03:40,513
the line W. It's given here.

47
00:03:40,513 --> 00:03:41,993
Next, we'll look at conjugate gradient method.

48
00:03:41,993 --> 00:03:43,520
This method benefits from using conjugacy information during optimization and

49
00:03:43,520 --> 00:03:47,141
usually requires far fewer steps.

50
00:03:47,141 --> 00:03:52,734
To converge, lets compact two batch gradient descent.

51
00:03:52,734 --> 00:03:54,630
The exact algorithm is given here.

52
00:03:54,630 --> 00:03:58,679
I'll skip the details of the algorithm and refer you

53
00:03:58,679 --> 00:03:59,399
to the key gradient.

54
00:03:59,399 --> 00:04:03,991
So, the exact algorithm is given here.

55
00:04:03,991 --> 00:04:13,088
And here is the DML script for conjugate gradient method.

56
00:04:13,088 --> 00:04:18,131
Like previous method, we'll plot the learned line.

57
00:04:18,131 --> 00:04:26,218
If you prefer not to write the custom algorithm,

58
00:04:26,218 --> 00:04:30,912
but instead invoke standard of the shelf algorithm,

59
00:04:30,912 --> 00:04:34,442
then you can use by ten cord given in example three and example four.

60
00:04:34,442 --> 00:04:37,747
In example three, we invoke,

61
00:04:37,747 --> 00:04:44,950
we implemented algorithm by using dmlFromResource method and ML context object.

62
00:04:44,950 --> 00:04:50,730
The pre-implemented algorithms have a label under script folder in our GitHub.

63
00:04:50,730 --> 00:04:58,300
All one has to do is create a script object from dmlFromResource,

64
00:04:58,300 --> 00:05:05,200
pass it the input feature and the response variable,

65
00:05:05,200 --> 00:05:10,900
that is X and Y using the input method.

66
00:05:12,350 --> 00:05:15,620
And like previous example,

67
00:05:15,620 --> 00:05:19,254
we'll plot the learned line.

68
00:05:19,970 --> 00:05:24,350
Example four is targeted for a scikit-learn user.

69
00:05:24,350 --> 00:05:28,480
A scikit-learn user may want to

70
00:05:28,480 --> 00:05:34,915
only create a linear regression object and call the fit method.

71
00:05:34,915 --> 00:05:42,818
The fit method accepts the input features and response variable as Numpy matrices.

72
00:05:42,818 --> 00:05:46,796
That user can simply use our mllearn API,

73
00:05:46,796 --> 00:05:50,950
mllearn API allows a Python programmer to invoke

74
00:05:50,950 --> 00:05:56,085
systemML'S algorithm using a scikit-learn like API,

75
00:05:56,085 --> 00:06:00,203
where the input data can be Numpy Arrays, scifi matrices,

76
00:06:00,203 --> 00:06:03,778
all Panda data frame as well as Spark's

77
00:06:03,778 --> 00:06:09,171
MLPipeline API where the input data is a Spark data frame.

78
00:06:09,171 --> 00:06:15,130
Since these APIs conform to MLPipeline's estimator interface,

79
00:06:15,130 --> 00:06:19,700
they can be used in tandem with ML as feature extractors,

80
00:06:19,700 --> 00:06:23,580
transformers, scoring, and cross validation classes.

81
00:06:23,580 --> 00:06:29,835
So, here's the fit method, predict method,

82
00:06:29,835 --> 00:06:37,514
and we'll plot the learned line.

83
00:06:37,514 --> 00:06:44,460
Next, we see how

84
00:06:44,460 --> 00:06:48,771
to use Keras inside SystemML.

85
00:06:48,771 --> 00:06:52,435
There are three different ways to implement a deep learning model

86
00:06:52,435 --> 00:06:56,650
in SystemML using the DML bodied ML library,

87
00:06:56,650 --> 00:06:59,304
using the experimental Caffe2DML API,

88
00:06:59,304 --> 00:07:03,055
and using the experimental Keras2DML API.

89
00:07:03,055 --> 00:07:10,090
Keras2DML and Caffe2DML accept the deep planning model expressed think

90
00:07:10,090 --> 00:07:16,632
Keras or Caffe former and then underneath genre T because learning DML script.

91
00:07:16,632 --> 00:07:22,280
In this example, we train a LeNet network using MNIST dataset.

92
00:07:22,280 --> 00:07:27,565
We first load the MNIST dataset,

93
00:07:27,565 --> 00:07:30,500
then we create LeNet using Keras API.

94
00:07:30,500 --> 00:07:35,210
Keras model given you has

95
00:07:35,210 --> 00:07:37,765
two convolution layers with

96
00:07:37,765 --> 00:07:42,622
ReLU activation and same padding with MaxPooling layer in between.

97
00:07:42,622 --> 00:07:48,730
They are followed by two densely connected layers with dropout.

98
00:07:48,730 --> 00:07:54,365
Once we have created the Keras model,

99
00:07:54,365 --> 00:07:58,055
we can pass it to SystemML using Keras2DML class.

100
00:07:58,055 --> 00:08:04,890
We can then call fit or predict method other than mllearn API.