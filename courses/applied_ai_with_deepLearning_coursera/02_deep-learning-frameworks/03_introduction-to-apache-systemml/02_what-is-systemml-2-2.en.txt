To invoke SystemML from command line, the user has to provide the location of the ML script via -f option. The command line and argument such as "$". And now, can be parsed via -nvargs option. The user can also specify the execution mode using -exec option. In most cases, you will end up using hybrid_spark mode or hybrid mode. Let's use the Linear Regression as an example and consider three cases: In the first case, the data resides in on HDFS or local file system. In the second case, we'll assume that SystemML is part the whole spark workflow and data is available as RDD or DataFrame. In the third case, we'll assume that SystemML is called by other Java program for scoring. Hence, the data is available as Java double array. The Linear Regression given here and available via this URL takes at least three parameters: X, Y and B. X refers to the location of input features, Y refers to the location of the response variables, and B refers to the location where SystemML should store the estimated regression coefficient. The first case can be further divided into three subcases. The first subcase, the data resides on local file system and the user wants to use single node backedn. In the second subcase, the data resides on HDFS, and the user wants to use hadoop backend. In the third subcase, the data resized on HDFS again, and the user wants to use spark backend. Let's examine the first subcase. The other user invoke SystemML using Java by providing the DMLScript, using -f option. And the named arguments X, Y and B, are provided using -nvargs option. It is important to note that the second command preserves algebraic rewrites, but now also spawn local MR jobs. If you want to run the same Linear Regression script without a single line of change on a hadoop, you'll invoke it with hadoop jar command. The argument will remain exactly the same except in this case you'll provide hybrid as the execution mode. If you want to run the Linear Regression script on spark cluster, you will use spark-submit command. In this case, you may not provide -exec option, as per SystemML will infer that you are running on spark cluster. Now let's examine the second case. Where SystemML is part of a spark workflow and data is available as an RDD or a DataFrame. Here, you can use the MLContext API that offers a programmer tech interface for interacting with SystemML from spark using languages such as Scala, Java, and Python. MLContext API allows the users to registers RDD's and DataFrame as input and output variables of a DMLScript. This enables SystemML to seamlessly integrate into a entire spark ecosystem. A MLContext object can be created by parsing its constructor, a reference of spark context or spark session. This ScripFactory class allows a DMLScript to be created from strings, files, URL's and input strings. Here, we will use DML from file method to create the script object. The input RDD X can be part using the inner input command. Finally, we execute the script using the execute method of MLContext object. The Python MLContext is similar to the Scala MLContext discribed earlier. SystemML's Python package is available on PyPi and can be installed using "pip" command. MLLearn API allows a Python programmer to invoke SystemML's algorithm using a scikit-learn like API or Spark's ML pipeline API. Hence, the input data can be a NumPy array, Scipy array, a Panda DataFrame, or a Spark DataFrame. Since these API conforms to the ML pipelines estimator interface, they can be used in tandem with MLLearn's feature extractors, transformers, coding and cross validation classes. The use of force created Linear Regression object given here, and then invokes fit and predict method. This shows the ML pipeline-like API where the input to the fit method is the DataFrame, and the next shows the scikit-like API where the input to the fit method are two NumPy arrays. There are three different ways to implement a deep learning modeling system. Using a DML bodied ML library, using the experimental Caffe2DML API, and using the experimental Keras2DML API. Keras2DML API and Caffe2DML API are instances of the MLLearn library we discussed earlier. Hence, they have fit and predict method and can take a Numpy array or a Spark DataFrame as input. Underneath, Keras2DML API takes a keras_model and generates an equivalent DML script. Similarly, Caffe2DML takes, as an input, a deep learning model expressed in caffe format and generates underneath the equivalent DML script. We will skip the JMLC's coding API in this video.