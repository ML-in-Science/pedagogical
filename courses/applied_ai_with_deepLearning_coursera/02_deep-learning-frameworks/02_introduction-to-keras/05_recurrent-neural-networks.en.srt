1
00:00:00,000 --> 00:00:02,405
All right. In this video,

2
00:00:02,405 --> 00:00:05,660
we're going to introduce the Recurrent Neural Networks in Keras,

3
00:00:05,660 --> 00:00:08,650
and in particular talk about an esteems.

4
00:00:08,650 --> 00:00:16,010
As an application, we're going to classify sentiments from movie reviews.

5
00:00:16,010 --> 00:00:22,153
So there's a few architectures available for Recurrent Neural Networks in Keras.

6
00:00:22,153 --> 00:00:24,545
The first one is a class called simpler RNN,

7
00:00:24,545 --> 00:00:28,520
which is a basic plane on the nose recurrent neural network,

8
00:00:28,520 --> 00:00:32,240
which suffers from problems like vanishing and exploding radiant.

9
00:00:32,240 --> 00:00:35,730
So you see those very rarely used in practice

10
00:00:35,730 --> 00:00:41,893
that gated recursive units introduced in 2014,

11
00:00:41,893 --> 00:00:44,515
has certainly its used cases.

12
00:00:44,515 --> 00:00:47,755
Also, an LSTM are

13
00:00:47,755 --> 00:00:53,535
a long short term memory models introduced in 1997 by Huffington Schmidhuber.

14
00:00:53,535 --> 00:00:59,009
Those are the most popular Recurrent Neural Networks in Keras in maybe overall.

15
00:00:59,009 --> 00:01:07,156
So, oftentimes when people talk about using RNNs usually they mean finding LSTMs.

16
00:01:07,156 --> 00:01:11,665
We'll focus in this lecture exclusively on LSTMs.

17
00:01:11,665 --> 00:01:16,560
Alright. How do we build LSTM layers with Keras?

18
00:01:16,560 --> 00:01:18,755
There's a few things we have to specify.

19
00:01:18,755 --> 00:01:22,441
Recall that with LSTM layer,

20
00:01:22,441 --> 00:01:25,185
you have two sets of weights.

21
00:01:25,185 --> 00:01:31,125
First, regular set of weights and then also recurrent set of weights.

22
00:01:31,125 --> 00:01:36,025
So in Keras that's called recurrent RNN.

23
00:01:36,025 --> 00:01:37,700
And apart from specifying the units,

24
00:01:37,700 --> 00:01:41,347
so the number of units and then activation fronting for all layer,

25
00:01:41,347 --> 00:01:45,020
we also have to specify everything recurrent.

26
00:01:45,020 --> 00:01:50,348
So for instance we need a recurrent initializer or recurrent activation and so on.

27
00:01:50,348 --> 00:01:56,410
So if you want to specialize or specify a regularizer or constraint, you can do that.

28
00:01:56,410 --> 00:02:00,475
Something of note here is that LSTM layers

29
00:02:00,475 --> 00:02:03,825
allows you to specify a dropout within the layer.

30
00:02:03,825 --> 00:02:07,600
So you don't have to specify a dropout layer separately,

31
00:02:07,600 --> 00:02:10,990
but you can specify a dropout rate for a LSTM.

32
00:02:10,990 --> 00:02:14,380
Both for direct set of weights,

33
00:02:14,380 --> 00:02:20,260
specified by dropout and for the recurrent weights specified by recurrent neural network.

34
00:02:20,260 --> 00:02:23,950
The last key words are we mention here is

35
00:02:23,950 --> 00:02:28,275
return sequences which are set True, False, by default.

36
00:02:28,275 --> 00:02:32,588
So, what that means is if you started to true,

37
00:02:32,588 --> 00:02:38,780
then return of your LSTM won't be a simple vector but a matrix instead.

38
00:02:38,780 --> 00:02:44,470
So, where you run an LSTM is by you apply different time sets.

39
00:02:44,470 --> 00:02:48,920
Right? You go through different time sets and at the end the output is one,

40
00:02:48,920 --> 00:02:50,790
one back to usual.

41
00:02:50,790 --> 00:02:52,521
If you set return sequences to true,

42
00:02:52,521 --> 00:02:54,310
you will also get returns,

43
00:02:54,310 --> 00:03:00,170
all at the immediate values adding up at the matrix time.

44
00:03:00,300 --> 00:03:06,030
Okay. To compute our application we need to introduce

45
00:03:06,030 --> 00:03:12,090
another layer which is quite useful for many such applications.

46
00:03:12,090 --> 00:03:14,200
And so embedding letters,

47
00:03:14,200 --> 00:03:16,950
are used as a very first layer in Keras.

48
00:03:16,950 --> 00:03:22,880
And what you use them for is you transform integers into vectors and like.

49
00:03:22,880 --> 00:03:25,110
To you give you one example,

50
00:03:25,110 --> 00:03:27,450
these two numbers are three and 12,

51
00:03:27,450 --> 00:03:30,680
those can be embedded into vectors of length two,

52
00:03:30,680 --> 00:03:32,670
which are on the right.

53
00:03:32,670 --> 00:03:38,175
A prototypical example that you would let us join which is going to do,

54
00:03:38,175 --> 00:03:45,120
is we want to embed a certain vocabulary into a vector space,

55
00:03:45,120 --> 00:03:48,940
meaning each word in those vocabulary has to be

56
00:03:48,940 --> 00:03:53,833
embedded or has to be given a representation of a vector space,

57
00:03:53,833 --> 00:03:59,070
and then you apply those embedments to sequences of words.

58
00:03:59,070 --> 00:04:05,710
This is when you can map a 2D input to a 3D output which connects to LSTM.

59
00:04:05,710 --> 00:04:11,675
So meaning, if you have a mini-batch of sequences of IDs,

60
00:04:11,675 --> 00:04:21,410
you can map those to mini-batch of sequences of vectors by a mini-batch of matrices.

61
00:04:21,410 --> 00:04:26,840
And this can be set into an LSTM.

62
00:04:26,840 --> 00:04:30,255
Alright. How do we initialize on embedding with Keras?

63
00:04:30,255 --> 00:04:32,870
There's a few things we have to specify.

64
00:04:32,870 --> 00:04:35,420
First off, the input dimension is

65
00:04:35,420 --> 00:04:39,755
our vocabulary size that we have to define before length.

66
00:04:39,755 --> 00:04:43,970
We also need to specify the output vector length, of the day.

67
00:04:43,970 --> 00:04:46,230
In our example from the previous lines,

68
00:04:46,230 --> 00:04:49,540
which show two dimensional vectors that usually returns

69
00:04:49,540 --> 00:04:54,500
much larger vectors as I put in dimension.

70
00:04:54,500 --> 00:04:56,250
Third item we need to specify here,

71
00:04:56,250 --> 00:04:58,385
is this so called embeddings initializes.

72
00:04:58,385 --> 00:05:02,580
So the weights of embedding in

73
00:05:02,580 --> 00:05:08,335
Keras are called Embeddings Voids and we initialize them like that.

74
00:05:08,335 --> 00:05:14,480
A false key word what's quite interesting is the mask zero flag.

75
00:05:14,480 --> 00:05:20,430
So, your input sequences in an embedding layer,

76
00:05:20,430 --> 00:05:23,195
may or may not have different lengths.

77
00:05:23,195 --> 00:05:27,650
So, if you think about sentences of different length,

78
00:05:27,650 --> 00:05:36,205
and we can use the value zero as a special value that we can then mask out.

79
00:05:36,205 --> 00:05:41,110
So for instance, you start out with sequences of various length,

80
00:05:41,110 --> 00:05:48,815
and then you paart those sequences with zeros to make them of the same size.

81
00:05:48,815 --> 00:05:54,197
Only do them mask out the zero values.

82
00:05:54,197 --> 00:05:58,031
Okay.

83
00:05:58,031 --> 00:06:02,650
Our use case here is sentiment classification from movie reviews,

84
00:06:02,650 --> 00:06:08,660
which is a database of 25000 movie reviews from IMDB.

85
00:06:08,660 --> 00:06:13,700
Those are labeled either as good or bad.

86
00:06:13,700 --> 00:06:18,140
The data status is also visible through the Keras dataset module.

87
00:06:18,140 --> 00:06:25,385
And the good thing is that the data is already preprocessed sequences of integers.

88
00:06:25,385 --> 00:06:28,935
So, we don't actually have to work with

89
00:06:28,935 --> 00:06:35,293
the words vocabulary but those have already been marked as integers.

90
00:06:35,293 --> 00:06:41,775
Our task then is to classify our sentiment so good or bad from the review content.

91
00:06:41,775 --> 00:06:48,410
And the way we are going to tackle this is we first embed our sentences with

92
00:06:48,410 --> 00:06:54,910
an embedding layer and then learn the sequential structure with an LSTM.

93
00:06:54,910 --> 00:06:58,237
So how do we do this?

94
00:06:58,237 --> 00:07:04,950
So we have a few imports of sequential model maxlen layer,

95
00:07:04,950 --> 00:07:11,255
embedding layer and LSTM and we also import our data set.

96
00:07:11,255 --> 00:07:17,610
So first off, we spent specifying the maximum number of features that goes to 20,000.

97
00:07:17,610 --> 00:07:27,455
So meaning, we only choose to model 20000 most common items from our vocabulary.

98
00:07:27,455 --> 00:07:32,110
We specify a maximum length for our features.

99
00:07:32,110 --> 00:07:38,205
So, we only allow sequences of length 80 which would be quite short.

100
00:07:38,205 --> 00:07:42,030
With movie reviews, that's how we choose.

101
00:07:42,030 --> 00:07:46,350
So then we can simply load IMDB data into memory,

102
00:07:46,350 --> 00:07:49,260
by imposing IMDB low data with

103
00:07:49,260 --> 00:07:55,160
the maximum of features as specified to get back training and test data.

104
00:07:55,160 --> 00:08:02,840
Alright. Next, we pad on sequences according to the maxlength we have specified.

105
00:08:02,840 --> 00:08:06,575
So that would mean if the sequence training and

106
00:08:06,575 --> 00:08:10,735
testing of data sets is shorter than 80 items,

107
00:08:10,735 --> 00:08:14,590
we would pad them with zeros at the end,

108
00:08:14,590 --> 00:08:16,300
and if they're longer, we crack them.

109
00:08:16,300 --> 00:08:23,430
Next, we initialize the sequential model and add on

110
00:08:23,430 --> 00:08:33,080
the embedding layer with the given maximum number of features and 128 equal dimension.

111
00:08:33,080 --> 00:08:40,270
This then connects to LSTM layer with also 128 equal dimension,

112
00:08:40,270 --> 00:08:47,050
and we also specify 20 percent dropout rate for both regular and recurring weights.

113
00:08:47,050 --> 00:08:51,875
At very end, we are at the dense layer of the single output dimension and

114
00:08:51,875 --> 00:08:57,050
the sigmoid function which would specify a value between zero and one,

115
00:08:57,050 --> 00:09:01,009
zero meaning bad and one good.

116
00:09:01,009 --> 00:09:06,570
What's left is to compile them all which we again do with binary crossentropy,

117
00:09:06,570 --> 00:09:13,385
suppress the gradient and we also have the same accuracy as before.

118
00:09:13,385 --> 00:09:19,925
Then we set our model with batch size 32 and let it run for 15 epochs,

119
00:09:19,925 --> 00:09:23,510
and again we use our test service validation data.

120
00:09:23,510 --> 00:09:26,810
Afterwards, we can evaluate our model.