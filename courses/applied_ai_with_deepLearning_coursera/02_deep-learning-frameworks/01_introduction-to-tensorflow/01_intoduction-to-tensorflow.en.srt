1
00:00:00,610 --> 00:00:03,890
TensorFlow is an open-source library in

2
00:00:03,890 --> 00:00:07,370
numerical computation and it's using data flow graphs.

3
00:00:07,370 --> 00:00:09,380
It allows us to express machine learning and

4
00:00:09,380 --> 00:00:13,055
deep learning algorithms and prints along an execution engine,

5
00:00:13,055 --> 00:00:15,100
which allows these algorithms to run at scale

6
00:00:15,100 --> 00:00:18,374
on multiple nodes in a cluster backed by CPUs,

7
00:00:18,374 --> 00:00:22,285
GPUs, TPU's and mobile devices.

8
00:00:22,285 --> 00:00:25,310
So every numerical computation is a graph,

9
00:00:25,310 --> 00:00:26,930
better note that computations,

10
00:00:26,930 --> 00:00:29,555
and on the excess out flowing 10 slots between them,

11
00:00:29,555 --> 00:00:31,360
they are from in tens of flow.

12
00:00:31,360 --> 00:00:35,325
So let's start with the little decoding examined within IBM data center experience.

13
00:00:35,325 --> 00:00:37,505
We create a new notebook.

14
00:00:37,505 --> 00:00:41,590
We give you the name and select the correct spark service although,

15
00:00:41,590 --> 00:00:43,200
we do not need it at the moment.

16
00:00:43,200 --> 00:00:45,070
And then we click on Create.

17
00:00:45,070 --> 00:00:48,905
Make sure that Python is selected as programming language.

18
00:00:48,905 --> 00:00:51,140
The first of all, we download some data.

19
00:00:51,140 --> 00:00:53,980
TensorFlow has a couple of built in datasets and

20
00:00:53,980 --> 00:00:57,335
reduce the endless handwritten digit classifications set.

21
00:00:57,335 --> 00:01:03,355
Now the data gets downloaded and we obtained a complex Python object called MNIST.

22
00:01:03,355 --> 00:01:08,170
Let's import TensorFlow first and then examine the data we've just downloaded.

23
00:01:08,170 --> 00:01:12,480
We used the jupiter metric command matplotlib inline,

24
00:01:12,480 --> 00:01:15,775
to display to plots directly in the notebook.

25
00:01:15,775 --> 00:01:19,330
Then we accessed the first image out of the

26
00:01:19,330 --> 00:01:23,350
55,000 from the MNIST training set using a built-in iterator,

27
00:01:23,350 --> 00:01:25,625
which is quite useful for later usage,

28
00:01:25,625 --> 00:01:28,600
but now we just accessed one image at a time.

29
00:01:28,600 --> 00:01:32,910
Images in the MNIST data set are 28 by 28 pixels,

30
00:01:32,910 --> 00:01:37,305
but we are only obtaining a vector of 784 pixel's length.

31
00:01:37,305 --> 00:01:39,730
Therefore, we have to reshape accordingly.

32
00:01:39,730 --> 00:01:42,120
Then we tell pyplot to plot,

33
00:01:42,120 --> 00:01:44,735
create, scale, and finally plot the image.

34
00:01:44,735 --> 00:01:47,173
So this is a three,

35
00:01:47,173 --> 00:01:50,030
and if we run again, we see a one.

36
00:01:50,030 --> 00:01:53,410
Note that every time we call next batch, we obtain a new image.

37
00:01:53,410 --> 00:01:56,100
Sometimes, we are not really sure but number is meant,

38
00:01:56,100 --> 00:01:58,750
therefore we can also print the label.

39
00:01:58,750 --> 00:02:01,520
This is a one hot encoded vector,

40
00:02:01,520 --> 00:02:04,665
this means at the index of the actual corresponding number,

41
00:02:04,665 --> 00:02:08,335
we see one, and all the other elements are zero.

42
00:02:08,335 --> 00:02:10,620
Now let's start with Tensorflow coding by

43
00:02:10,620 --> 00:02:13,970
expressing the state to computation graph using Python.

44
00:02:13,970 --> 00:02:16,740
We start with a so-called placeholder.

45
00:02:16,740 --> 00:02:21,340
This is a tense service in Tensorflow that data is fed in during execution time.

46
00:02:21,340 --> 00:02:24,280
So basically, this is used to add data during training

47
00:02:24,280 --> 00:02:28,465
which takes place after this computation graph is constructed.

48
00:02:28,465 --> 00:02:30,750
Those placeholders are typed,

49
00:02:30,750 --> 00:02:33,795
and we can use either single or double position.

50
00:02:33,795 --> 00:02:36,909
So this placeholder would take our training vectors,

51
00:02:36,909 --> 00:02:40,900
representing the images, the 784 elements inside.

52
00:02:40,900 --> 00:02:44,205
Now will create Tensorflow variable.

53
00:02:44,205 --> 00:02:47,200
A variable is something Tensorflow retrieve during training,

54
00:02:47,200 --> 00:02:50,415
whereas the placeholder is meant to keep training data.

55
00:02:50,415 --> 00:02:53,100
In addition, a variable can be saved to disk during and

56
00:02:53,100 --> 00:02:56,383
after training for check pointing and water transfer.

57
00:02:56,383 --> 00:03:01,785
So we create our wait matrix W with 784 Baitz on one X's.

58
00:03:01,785 --> 00:03:04,155
Just one for each element of X,

59
00:03:04,155 --> 00:03:05,930
and we do it 10 times.

60
00:03:05,930 --> 00:03:09,615
Since you are basically running 10 soft mix regression motors in parallel,

61
00:03:09,615 --> 00:03:11,790
one for each possible digit.

62
00:03:11,790 --> 00:03:17,180
Finally, we end up with a bias that draw one foot each soft next regression model.

63
00:03:17,180 --> 00:03:19,250
Now we create the actual model.

64
00:03:19,250 --> 00:03:23,270
So please be aware that no computation is happening at this stage.

65
00:03:23,270 --> 00:03:29,285
We are not basically hooking up the notes together to form a computation of graph.

66
00:03:29,285 --> 00:03:35,460
Softmax as well as Matmul expect Tensorflow variables as Polanyi does.

67
00:03:35,460 --> 00:03:39,150
So that's the reason TensorFlow court is so hard to read.

68
00:03:39,150 --> 00:03:40,930
We are not computing anything,

69
00:03:40,930 --> 00:03:44,010
we just despite the expressive computation of graph,

70
00:03:44,010 --> 00:03:47,675
which is executed by the TensorFlow engine in the background.

71
00:03:47,675 --> 00:03:51,720
TensorFlow is not having it's own domain specific language for doing so,

72
00:03:51,720 --> 00:03:55,580
but it's relying on language bindings in different programming languages,

73
00:03:55,580 --> 00:03:57,880
like python for example.

74
00:03:57,880 --> 00:04:01,440
System ML on the other hand, which is introduced in another module is a bit better here,

75
00:04:01,440 --> 00:04:06,725
since System ML is having its own domain specific language and are syntax of Python,

76
00:04:06,725 --> 00:04:09,085
which looks found more nature.

77
00:04:09,085 --> 00:04:14,155
Anyway, let's continue with creating another placeholder for a training level spike.

78
00:04:14,155 --> 00:04:16,710
These are often mentioned 10, because why?

79
00:04:16,710 --> 00:04:20,050
It is the one hot encoded vector labeling the image.

80
00:04:20,050 --> 00:04:22,945
Now we defined the cost function as cross-entropy.

81
00:04:22,945 --> 00:04:26,065
Therefore, let me just walk you through the formula and we will see

82
00:04:26,065 --> 00:04:30,025
later how to implement it in TensorFlow.

83
00:04:30,025 --> 00:04:34,700
So we take a predictive value of Y head and multiply

84
00:04:34,700 --> 00:04:41,220
it to the log of the desired value of Y and some of those values up.

85
00:04:41,220 --> 00:04:46,645
So we start with the reduce mean function of TensorFlow,

86
00:04:46,645 --> 00:04:50,622
because we are now calculating 10 individual cross-entrophy values,

87
00:04:50,622 --> 00:04:53,540
one for each softmax regression model.

88
00:04:53,540 --> 00:04:56,313
Then we use Reduce Sum,

89
00:04:56,313 --> 00:05:00,100
to calculate the sum of the individual values of a Tensor.

90
00:05:00,100 --> 00:05:02,855
And this Tensor is the product of the desired value,

91
00:05:02,855 --> 00:05:05,515
and the luck of the actual prediction.

92
00:05:05,515 --> 00:05:08,330
Reduction indices defines that

93
00:05:08,330 --> 00:05:11,935
the dimension of the Tensor that aggregation should take place.

94
00:05:11,935 --> 00:05:15,260
Since Y is a matrix of 10 columns and N rows,

95
00:05:15,260 --> 00:05:17,945
the N stands for a number of creating examples,

96
00:05:17,945 --> 00:05:21,815
the sum over the columns to obtain the value for each digit.

97
00:05:21,815 --> 00:05:25,730
This reside has no past to an argument to reduce mean,

98
00:05:25,730 --> 00:05:29,365
so that the overall prediction error is calculated

99
00:05:29,365 --> 00:05:34,195
all of the individual prediction errors for each number between zero and nine.

100
00:05:34,195 --> 00:05:40,081
Now, we use TensorFlow GradientDescentOptimizer with the learning rate of zero to five,

101
00:05:40,081 --> 00:05:44,090
to tweak W and B with respect to the cross-entropy function.

102
00:05:44,090 --> 00:05:46,880
So TensorFlow will take care of calculating

103
00:05:46,880 --> 00:05:50,420
the back propagation and gradients for this task automatically.

104
00:05:50,420 --> 00:05:53,330
A feature called automatic differentiation,

105
00:05:53,330 --> 00:05:56,195
does the job for us.

106
00:05:56,195 --> 00:05:58,055
Now we create a TensorFlow session,

107
00:05:58,055 --> 00:06:00,050
since we are in an interactive context within

108
00:06:00,050 --> 00:06:03,050
a Jupiter notebook we use the interactive session.

109
00:06:03,050 --> 00:06:07,233
A session is the way to deploy a TensorFlow execution graph,

110
00:06:07,233 --> 00:06:12,050
onto a specific execution context like a CPU or GPU.

111
00:06:12,050 --> 00:06:15,050
Then we initialize all global variables,

112
00:06:15,050 --> 00:06:16,710
since this hasn't been done.

113
00:06:16,710 --> 00:06:20,845
Remember, the chest has only expressed the computation of graph.

114
00:06:20,845 --> 00:06:22,840
Now it's time to bring it to life.

115
00:06:22,840 --> 00:06:25,440
After the variables have been initialized,

116
00:06:25,440 --> 00:06:28,500
it's time to create our GradientsDescent loop.

117
00:06:28,500 --> 00:06:30,338
So this is batch Gradienst Descent,

118
00:06:30,338 --> 00:06:31,890
since on each iteration,

119
00:06:31,890 --> 00:06:34,845
we graph a hand that randomly selected examples

120
00:06:34,845 --> 00:06:38,163
from returning set and using the session object,

121
00:06:38,163 --> 00:06:42,595
the ExecuteGradientDescent for those hand out examples.

122
00:06:42,595 --> 00:06:47,000
Note that if you pass the training example as parameters to this function call,

123
00:06:47,000 --> 00:06:50,655
in order to assign them to the previously defined placeholders.

124
00:06:50,655 --> 00:06:53,220
So this runs very fast.

125
00:06:53,220 --> 00:06:58,435
Now let's evaluate our classification performance using the test set from MNIST.

126
00:06:58,435 --> 00:07:01,420
So argmax returns the index of the tensor,

127
00:07:01,420 --> 00:07:03,460
in this case a vector,

128
00:07:03,460 --> 00:07:04,930
which the maximum value.

129
00:07:04,930 --> 00:07:09,805
This may be can transform back from one hot encoding scheme to a scaler.

130
00:07:09,805 --> 00:07:14,190
We use reduce mean in order to determine the amount of correctly predicted values,

131
00:07:14,190 --> 00:07:17,110
but since correct prediction is a full in vector,

132
00:07:17,110 --> 00:07:21,955
we have to cast it to float in order to calculate the mean over this vector.

133
00:07:21,955 --> 00:07:26,255
And again, accuracy is a note in computation of graph.

134
00:07:26,255 --> 00:07:30,660
Therefore, we need to use the TensorFlow session in order to execute it.

135
00:07:30,660 --> 00:07:32,620
Now the placeholders does become handy,

136
00:07:32,620 --> 00:07:35,585
because now we assign the test dataset to the graph.

137
00:07:35,585 --> 00:07:42,650
And as expected, this timber regression model gives us 92 percent of accuracy.

138
00:07:42,650 --> 00:07:46,555
So as you've seen training of new networks can be quite complicated.

139
00:07:46,555 --> 00:07:48,670
But it's okay if you didn't understand all.

140
00:07:48,670 --> 00:07:50,940
In the next module,

141
00:07:50,940 --> 00:07:52,641
we will cover TensorBoard,

142
00:07:52,641 --> 00:07:56,000
which is a way of debacking your new netbook.