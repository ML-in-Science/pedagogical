Hi. In this section, I am going to demonstrate some of the skills that you will need to execute Java code, particularly deep learning for J Neural network code in the data science experience. So, I've logged in to the data science experience, and the first thing I'm going to do is I'm going to create a new notebook. The notebook I'm going to create, I'm going to call Test DL4J. I'm going to use the Spark as a service, Python II, and create that notebook. Now, that the Kernel's started, let's go ahead and see what resources we already have available. So, I'm just going to run that LS command to list the contents of the current directory. The tools that we're going to be downloading, can be gathered from this URL. So, github.com/SkymindIO/dsx, for data science experience. In particular, we're going to go to the release, so that we can download these large files. This DL4J snapshot with dependencies.jar is all of the compiled classes in this repo. If you wanted to explore the Java, you could find that here in src/main/java/skymind/dsx. And there's the code for the classes that we're going to be executing. In this case, I'm going to execute the Iris-Classifier. And we'll see that the Iris-Classifier requires that we are able to read a file called iris.txt. So, in order to execute this, I will need two things. I will need the iris.txt file, that's up here in the top of the directory. And then I will need the compiled jar, that's in the release right there. So, let's grab that jar file, and let's grab that data file, iris.txt, and let's execute them in a data science experience. So, let's start by downloading the jar file. So, go to SkymindIO on GitHub, to the dsx project, there's one release. Right-click to get the URL. And then over in the data science experience, just execute bang wget to download that. We will also need the iris.txt file. And to locate that, it's in the top level of the project. There it is, iris.txt. You want to view the Raw, and then get that URL. And then over here, in the data science experience, wget with an exclamation point before it, and that URL. When those two commands have completed, we should see the jar file, and we should see iris.txt, and we do. So, we now have everything we need to execute it. One way to execute it is just use the local Java. If I type java version, bang java version, exclamation point java version, we see we have Java locally. And then, to execute that, I would just type "I need to specify the class path to include the jar," and then "I need to specify the class that I want to execute in that jar." And this is the class. So, if I execute that code, we will see the output of that. It's going to take a little bit of time, but we see it's building the model, and then it will go through many iterations on that model. As the model finishes, I will see it goes through thousand iterations. We see the score is generally decreasing, getting towards zero, meaning the model's getting better at making predictions. And when the model's done, we operate an evaluation of the true table, how accurate we were. So, Class zero, we got correct. Class one, we got correct 22 times. Class one, we got incorrect two times. And class two, we got correct 10 times. So, there you have executing the model using the local Java. I could also submit the jar and my data file to Spark. I would do that using this command. So, Spark homes an environmental variable that points to the location of the spark-submit executable. So, I start with that, and once again, I start to command with an exclamation point. Spark-submit allows you to specify which class you're going to execute, so I specify that here. When I'm submitting a job to Spark, I need to specify the master, and once again, the data science experience has an environmental variable set named Master that points to the URL of the current Spark Master. We need to ship it the file iris.txt and we can do that using the files command. And then, the last argument, we need to include the jar. So, this command will say, "Ship this jar up to this Spark master, execute this class." That class will be looking for this file, iris.txt, and that command, --files, ships it up and makes it available to each worker. Note that here, we're really just using Spark as an execution environment. We're not building a Spark context, doing fully distributed work. We're just shipping a single class to execute once, so we're not taking full advantage of Spark, but we are executing our code in Spark. And there you see the output, as it begins to run. As that code finishes, you'll see the same thing. You'll see the score at the final iteration, you'll see the truth table and there you have it. So, we can execute our Java code in two ways. In both cases, we need to build a Hoover Jar, a jar with dependencies, ship the jar and any needed text files into the data science experience. Once we have that content in the data science experience, we can either execute Java, ship it to class path of the jar, specify the class you want to execute and execute it locally in the data science experience, or we can ship the jar and specify the class to execute when we ship that to spark-submit. Thank you.