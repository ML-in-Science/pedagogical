जब आप बनाते हैं एक न्यूरल नेटवर्क एक विकल्प जो आपको चुनना होता है कि क्या ऐक्टिवेशन फ़ंक्शन इस्तेमाल करना है हिडन लेयर्स में तथा आउट्पुट यूनिट में आपके न्यूरल नेटवर्क में. अभी तक हम केवल इस्तेमाल करते रहे हैं सिग्मोईड ऐक्टिवेशन फ़ंक्शन लेकिन कभी कभी अन्य विकल्प 
कर सकते हैं बेहतर काम. चलो देखते हैं कुछ विकल्प फ़ॉर्वर्ड प्रॉपगेशन स्टेप में न्यूरल नेटवर्क में. हमारे पास हैं ये दो स्टेप्स जहाँ हम इस्तेमाल 
करते हैं सिग्मोईड फ़ंक्शन यहाँ तो उस सिग्मोईड को कहते हैं एक ऐक्टिवेशन फ़ंक्शन और यहाँ है परिचित सिग्मोईड फ़ंक्शन a बराबर 1 ओवर 1 प्लस e की पॉवर माइनस z तो अधिक सामान्य केस में हमारे पास 
हो सकता है एक फ़ंक्शन G, z का. चलो लिखते हैं इसे यहाँ जहाँ G हो सकता हैं एक नॉन-लिनियर फ़ंक्शन जो हो सकता है न हो सिग्मोईड फ़ंक्शन. तो उदाहरण के लिए सिग्मोईड फ़ंक्शन होता है 0 और 1 के बीच. एक ऐक्टिवेशन फ़ंक्शन जो लगभग हमेशा बेहतर काम करता है सिग्मोईड फ़ंक्शन से है tanh फ़ंक्शन या हायपर्बालिक टैंजेंट फ़ंक्शन तो यह है G. यह है a यह है a बराबर tanh z का और यह जाता है प्लस 1 और माइनस 1 के बीच. फ़ॉर्म्युला tanh फ़ंक्शन का है e की पॉवर z माइनस e की पॉवर नेगेटिव z ओवर उनका योग और 
यह है वास्तव में गणितीय रूप से एक शिफ़्ट किया हुआ वर्ज़न सिग्मोईड फ़ंक्शन का तो जैसे एक आप जानते हैं सिग्मोईड फ़ंक्शन बस उसके जैसा लेकिन शिफ़्ट किया हुआ ताकि अब यह
 क्रॉस करता है ज़ीरो, ज़ीरो पोईँट और दोबारा स्केल करके 
ताकि यह जाता है G पर माइनस एक और प्लस एक पर और ऐसा होता है के हिडन यूनिट्स के लिए यदि आप 
रखते हैं फ़ंक्शन G, z का बराबर z के tanh के, यह लगभग हमेशा 
बेहतर काम करता है सिग्मोईड फ़ंक्शन की तुलना में क्योंकि माइनस एक और प्लस एक के बीच की वैल्यूज़ से ऐक्टिवेशन्स की औसत जो आती है हिडन लेयर्स से वहाँ है क़रीब ज़ीरो होने के और बस जैसे कभी-कभी आप ट्रेन करते हैं एक लर्निंग अल्गोरिद्म आप डेटा को केंद्रित कर सकते हैं 
और कर सकते हैं आपका डेटा ज़ीरो औसत वाला इस्तेमाल करके 
एक tanh बजाय एक सिग्मोईड फ़ंक्शन के. इसका एक तरह से प्रभाव है केंद्रित करना आपका डेटा ताकि डेटा की औसत है क़रीब ज़ीरो के बजाय शायद एक 0.5 के और यह वास्तव में बनाता है लर्निंग अगली लेयर के लिए थोड़ा आसान. हम कहेंगे अधिक इस बारे में दूसरे कोर्स में जब हम बात करेंगे ऑप्टिमायज़ेशन अल्गोरिद्म्स की भी. लेकिन एक चीज़ समझने की है कि मैं लगभग कभी भी इस्तेमाल नहीं करता सिग्मोईड ऐक्टिवेशन फ़ंक्शन अब. tanh फ़ंक्शन है लगभग हमेशा काफ़ी बेहतर. एक अपवाद है आउट्पुट लेयर के लिए, क्योंकि यदि Y है 0 या 1 तब यह बेहतर लगता है कि y हैट हो एक नम्बर जो आप चाहते हैं आउट्पुट करना 0 और 1 के बीच बजाय प्लस 1 और माइनस 1 के बीच. तो एक अपवाद जहाँ मैं इस्तेमाल करूँगा सिग्मोईड ऐक्टिवेशन फ़ंक्शन जहाँ आप इस्तेमाल कर रहे हैं बाइनरी वर्गीकरण उस स्थिति में आप शायद इस्तेमाल करें सिग्मोईड ऐक्टिवेशन फ़ंक्शन आउट्पुट लेयर के लिए. तो z का G 2 यहाँ है बराबर सिग्मोईड z का 2 और इसलिए जो आप इस उदाहरण में है जहां आपके पास है शायद एक tanh ऐक्टिवेशन फ़ंक्शन हिडन लेयर के लिए और सिग्मोईड फ़ंक्शन आउट्पुट लेयर के लिए. तो ऐक्टिवेशन फ़ंक्शन्स भिन्न हो सकते हैं भिन्न लेयर्स के लिए और कभी-कभी डिनोट करने के लिए कि ऐक्टिवेशन फ़ंक्शन्स है भिन्न भिन्न लेयर्स के लिए हम शायद उपयोग करें इन वर्ग कोष्ठक सूपरस्क्रिप्ट्स का भी इंगित करने के लिए कि G वर्ग कोष्ठक 1 है भिन्न G वर्ग कोष्ठक 2 से. ठीक है, फिर से वर्ग कोष्ठक 1 सूपरस्क्रिप्ट संदर्भित करता है इस लेयर को और सुपरस्क्रिप्ट वर्ग कोष्ठक 2 संदर्भित करता है आउट्पुट लेयर को. अब एक नकारात्मक पहलू दोनो सिग्मोईड फ़ंक्शन और tanh फ़ंक्शन का है कि यदि z है या बहुत बड़ा या बहुत छोटा तब ग्रेडीयंट डेरिवेटिव का या इस फ़ंक्शन की स्लोप हो जाती है बहुत छोटी तो z है बहुत बड़ा या z है बहुत छोटा तो स्लोप फ़ंक्शन की आप जानते हैं हो जाती है क़रीब ज़ीरो और इसलिए यह धीमा कर सकता है ग्रेडीयंट डिसेंट. तो एक विकल्प जो बहुत लोकप्रिय है मशीन लर्निंग में है जिसे कहते हैं रेक्टिफ़ायड लिनियर यूनिट तो ReLU फ़ंक्शन ऐसा दिखता हैं और फ़ॉर्म्युला है a बराबर मैक्स ऑफ़ 0 कॉमा z इसलिए डेरिवेटिव है 1 जब तक z है पॉज़िटिव और डेरिवेटिव या स्लोप है 0 जब z है नेगेटिव. यदि आप इम्प्लमेंट कर रहे हैं इसे तकनीकी रूप से डेरिवेटिव जब Z है पूर्णत: 0 नहीं है परिभाषित लेकिन जब आप इम्प्लमेंट करते हैं इसे कम्प्यूटर में तब अक्सर आपको मिलेगा यह z पूर्णत:बराबर 0 0 0 0 0 0 0 0 0 0. यह बहुत ही कम है तो आपको ज़रूरत नहीं है चिंता करने की इसकी. व्यावहारिक रूप से आप ले सकते हैं एक डेरिवेटिव जब z है बराबर 0 आप इसे ले सकते हैं 1 या 0 और आपका काम होगा सही. तो तथ्य कि इसे ड़िफ़्फ़ेरेंशिएट नहीं कर सकते तो यहाँ हैं कुछ अनुभवसिद्ध नियम चुनने के लिए ऐक्टिवेशन फ़ंक्शन्स यदि आपकी आउट्पुट है 0 1 वैल्यू यदि आप इस्तेमाल कर रहे हैं बाइनरी वर्गीकरण तब सिग्मोईड ऐक्टिवेशन फ़ंक्शन बहुत स्वभाविक है आउट्पुट लेयर के लिए और तब बाक़ी सभी यूनिट्स के लिए ReLU या रेक्टिफ़ायड लिनियर यूनिट बन रहा है तेजी से डिफ़ॉल्ट विकल्प ऐक्टिवेशन फ़ंक्शन का तो यदि आप नहीं हैं सुनिश्चित कि क्या इस्तेमाल करें 
आपकी हिडन लेयर के लिए मैं बस इस्तेमाल करूँगा ReLU ऐक्टिवेशन फ़ंक्शन. वह है जो आप देखेंगे अधिकांश लोग इस्तेमाल करते हैं इन दिनो जबकि कभी-कभी लोग इस्तेमाल करते हैं tanh ऐक्टिवेशन फ़ंक्शन भी. एक समस्या ReLU की है कि डेरिवेटिव है बराबर ज़ीरो के जब z है नेगेटिव. व्यावहारिक रूप से यह काम करता है सही लेकिन वहाँ एक अन्य वर्ज़न है ReLU का जिसे कहते हैं leaky ReLU. दूँगा आपको फ़ॉर्म्युला अगली स्लाइड पर लेकिन बजाय इसके होने के ज़ीरो जब z है नेगेटिव यह सिर्फ़ लेता है एक थोड़ी स्लोप इस तरह. तो इसलिए इसे कहते हैं leaky ReLU. यह अक्सर बेहतर काम करता है तुलना में ReLU ऐक्टिवेशन फ़ंक्शन के हालाँकि यह सिर्फ़ उतना इस्तेमाल नहीं होता अभ्यास में. 
कोई भी दोनो में से ठीक होना चाहिए हालांकि अगर आपको एक लेना होता मैं आमतौर पर लेता ReLU और लाभ दोनो ReLU और leaky ReLU का है कि बहुत सी स्पेस z की, डेरिवेटिव ऐक्टिवेशन फ़ंक्शन का, स्लोप ऐक्टिवेशन फ़ंक्शन की है बहुत अलग ज़ीरो से और इसलिए अभ्यास में इस्तेमाल करना ReLU ऐक्टिवेशन फ़ंक्शन आपका न्यूरल नेटवर्क अक्सर लर्न करेगा बहुत जल्दी तुलना में 
इस्तेमाल करने से tanh या सिग्मोईड ऐक्टिवेशन फ़ंक्शन के और मुख्य कारण है कि वहाँ इसका प्रभाव कम है स्लोप पर फ़ंक्शन के ज़ीरो होने पर जिससे धीरे हो जाती है लर्निंग और मैं जानता हूँ कि आधी रेंज के लिए z की स्लोप ReLU की है ज़ीरो लेकिन अभ्यास में पर्याप्त आपकी हिडन यूनिट्स में होगा z बड़ा ज़ीरो से तो लर्निंग रेट हो सकती है काफ़ी अधिक बहुत से ट्रेनिंग इग्ज़ाम्पल्ज़ के लिए. तो चलो जल्दी से देखते हैं लाभ और हानियाँ विभिन्न ऐक्टिवेशन फ़ंक्शन्स की. यहाँ है सिग्मोईड ऐक्टिवेशन मैं कहूँगा कभी इस्तेमाल न करें सिवाय आउट्पुट लेयर के यदि आप कर रहे हैं बाइनरी वर्गीकरण या शायद कभी न इस्तेमाल करें इसका और कारण कि मैं लगभग कभी 
उपयोग नहीं करता इसका है क्योंकि tanh है बहुत अधिक बेहतर. तो tanh ऐक्टिवेशन फ़ंक्शन है यह और फिर डिफ़ॉल्ट, सबसे अधिक इस्तेमाल होने वाला ऐक्टिवेशन फ़ंक्शन है ReLU जो है यह. तो आप सुनिश्चित नहीं है क्या और इस्तेमाल कर सकते हैं, इस्तेमाल करें इसे 
और शायद आप जानते हैं बेझिझक इस्तेमाल करें leaky ReLU. जहाँ शायद 0.01 z कॉमा z, ठीक है तो a है मैक्स 0.01 गुणा z और z का. तो वह देता है आपको यह मोड़ फ़ंक्शन में और आप शायद कहें आप जानते हैं क्यों है वह कॉन्स्टंट 0.01. ठीक है आप बना सकते हैं उसे एक अन्य पेरमिटर लर्निंग अल्गोरिद्म का और कुछ लोगों का कहना है कि वह काम करता है 
और भी बेहतर लेकिन मैं शायद ही लोगों को देखता हूँ 
ऐसा करते हुए तो लेकिन यदि आपको लगता है कोशिश करना चाहिए 
इसे आपकी ऐप्लिकेशन में आप जानते है कृपया बेझिझक करें वैसा और आप तब देख सकते हैं कैसे यह काम करता हैं और कितना अच्छा काम करता है 
और बने रहें साथ इसके यदि यह देता है अच्छे परिणाम. 
तो मुझे उम्मीद है कि वह देता है आपको एक समझ कुछ विकल्पों की ऐक्टिवेशन फ़ंक्शन्स के 
जो आप कर सकते हैं इस्तेमाल आपके नेटवर्क में. एक विषय हम देखेंगे डीप लर्निंग में है कि आपके पास अक्सर हैं बहुत से विभिन्न विकल्प कैसे आप बनाते हैं आपका न्यूरल नेटवर्क हिडन यूनिट्स की संख्या से लेकर ऐक्टिवेशन फ़ंक्शन के चुनाव तक कि कैसे आप लेते हैं प्रारम्भिक वेट्स जो हम देखेंगे बाद में. बहुत से विकल्प उस तरह के और ऐसा होता है कि कभी-कभी कठिन होता है पाना अच्छे दिशा निर्देश ठीक उसी के लिए जो करेंगे श्रेष्ठतम कार्य आपकी समस्या के लिए. तो इसलिए इन कोर्सेज़ के माध्यम से मैं देता रहूँगा आपको एक समझ उसकी जो मैं देखता हूँ इंडस्ट्री में कि क्या कम या अधिक लोकप्रिय है. लेकिन आपकी ऐप्लिकेशन के लिए आपकी ऐप्लिकेशन के लक्षणों के लिए यह है वास्तव में बहुत मुश्किल जानना शुरुआत में सही रूप से कि क्या काम करेगा 
सबसे अच्छा तो एक सलाह होगी कि यदि आप नहीं है सुनिश्चित कि कौन सा इन ऐक्टिवेशन फ़ंक्शन में काम करेगा सबसे अच्छा 
आप जानते हैं परीक्षण करें उन सबका और फिर आकलन करें उनका 
एक होल्डआउट वैलिडेशन सेट पर या एक डिवेलप्मेंट सेट पर जिसके बारे में हम बात करेंगे बाद में और देखें कौन सा काम करता है बेहतर और फिर चुने उसे और मुझे लगता है परीक्षण के द्वारा ये भिन्न विकल्प आपकी ऐप्लिकेशन के लिए आपको भविष्य अशुद्धि जांच में बेहतर होगा आपके न्यूरल नेटवर्क आर्किटेक्चर के मुक़ाबले में विलक्षणों के हमारी समस्या के भी. अल्गोरिद्म्स के विकास में बजाय आप जानते हैं यदि मैं आपको कहता हमेशा इस्तेमाल करें ReLU ऐक्टिवेशन और 
न इस्तेमाल करें अन्य कुछ. वह शायद करे या न करे अप्लाई आपकी समस्या पर जिस पर आप काम कर रहे हैं आप जानते हैं या या निकट भविष्य में या आने वाले समय में. ठीक है तो वह था विकल्प ऐक्टिवेशन फ़ंक्शन के. आपने देखे सबसे लोकप्रिय ऐक्टिवेशन फ़ंक्शन्स. वहां एक अन्य सवाल है जो कभी-कभी पूछा जाता है कि क्यों आपको ज़रूरत ही है इस्तेमाल करने की ऐक्टिवेशन फ़ंक्शन की. क्यों न सिर्फ़ हटा दें उसे. तो चलो बात करते हैं उसकी अगले वीडियो में और जहाँ आप देखेंगे क्यों न्यूरल नेटवर्क को चाहिए किसी तरह का नॉनलिनियर ऐक्टिवेशन फ़ंक्शन.