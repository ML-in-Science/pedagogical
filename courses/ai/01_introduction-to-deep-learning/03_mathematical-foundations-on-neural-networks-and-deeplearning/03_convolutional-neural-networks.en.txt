Let's start with convolutional neural networks. So, the task we are looking at now is image classification. So, an image can be seen as a very large spectra of pixel values. Let's consider a gray scale image first for simplicity. So, what we try to learn is the hidden function, f which makes our vector, to a scalar, c which represents the class. While scalar works for binary classification, it might be better to use a complete vector for multiclass specification. This way we can train a neural network to learn an arbitrary number of classes. So, by and resolving this task with an ordinary feedforward neural network, it turns out that an image with only a hundred by a hundred pixels you would need 10,000 weights in the weight matrix. So obviously, those can't be applied to larger images due to computational complexity. So, let's have a look at those two diagrams. So, f is the original function, where g is a function used for convolution. So, convolution means sliding over a function using the convolution function as a filter. There exists different types of such filters. In this case, the area under the function is used to create the output of the convolution. Please note, that also it looks like it's not a continuous sliding, it's more some sort of step over and the step size is called stride. So, let's imagine we have a five by five pixels monochrome image encoded in this five by five matrix. And let's apply a three by three convolution matrix with a stride of one. The convolution returns one in case both elements of the matrix are one and zero otherwise. We start with the first step. We convert the top-left part of the original matrix with the convolution matrix and we obtain the following result. So, the convolution matrix acts as a filter letting only values go through at positions where itself has a one. So, since the resulting matrix has four elements which are one, we put a four into the result matrix. Then we do it for the next step. Here we end up with three one's and six zero's. Then we complete the first row. Now, we start with the second row. Here we obtain two. I think you got the pattern the right. So, this convolutional matrix is also called a filter, and the values in it are learned from data during training. Note, that the filter normally doesn't only contain zero's and one's as in this toy example. So, actually, we are learning efficient filters. This four-way image classification example perfectly illustrates this. In this training dataset, there have been images of faces, cars, elephant, and chairs. After training, the first layer, the second layer, and the third layer have been sampled. Then, 24 images per class have been selected and shown to the neural network, and the activations have been visualized. What's important to notice is, that the first layer didn't learn anything meaningful, at least not meaningful to us. It just learned basic shapes like lines, circles, and rectangles. And more interestingly, that independently of which image class we're looking at, they all look the same. So, this is actually the same thing what the computer vision guys are doing, they are just creating intelligent filters. So, if you make them redundant, and by the away, this is exactly how the visual cortex in our brain works. Another important layer type in convolutional neural networks is called pooling layer. The idea is to reduce dimensionality without losing too much spatial information. This process is also called subsampling or down sampling. As in the convolutional layer, we're looking at subsets of the original matrix. And from those values, we just conclude a single one. So, if you just take the maximum value, this is called maxpooling, we end up with six. There are these other pooling methods like average, or sum, and so on. Again, I think you've got the pattern, right? So, this brings us to the topology of a convolutional neural network. In this case, this is used for image classification. Note, that a fully connected layer is nothing else than the ordinary feedforward layer. This is just a toy example. Normally, convolutional neural networks have many, many layers. We'll see a practical example later in the course, so we will skip this for now. Let's start with recurrent neural networks.