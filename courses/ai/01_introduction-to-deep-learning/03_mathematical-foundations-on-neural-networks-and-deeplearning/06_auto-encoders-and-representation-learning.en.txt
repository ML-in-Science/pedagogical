Autoencoders are, besides LSTMs, the most exciting neural network topology. They can do amazing things but let's first understand how they work. So autoencoders are like neural networks with a couple of exceptions. As in an ordinary neural network, we have an input layer where we present examples of our dataset as vectors, X. Usually neural networks are trained to find a hidden function, f which maps X to Y. But an autoencoder is different, it tries to map X to X'. Or in other words, it tries to reconstruct whatever vector, X it sees on the left-hand side on right-hand side. This is also known as the identity function. And as you know that neural networks are universal function approximators, it is obvious that the neural network can learn the identity function. But now here's the catch. As there are no direct connections between X and X', all data has to pass through layer, Z. Since layer, Z has less neurons than X, all data needs to flow through this so-called neural bottleneck. This leads to a sort of identity function which is resistant to noise and doesn't learn any irrelevant data. We will use autoencoders throughout the course, but let me show you some simple applications of it. Anomaly detection. Since an autoencoder can't learn irrelevant data, it would do a good job in learning the whole training set as good as possible. So whenever data is shown to an autoencoder which it hasn't seen before, it will do a best job in reconstructing it. This can be used for anomaly detection. Dimension reduction is one of the most famous unsupervised Machine Learning disciplines, and prominent algorithms are PCA which stands for principle component analysis, or t-SNE which is t-Distributed Stochastic Neighbor Embedding. Whereas PCA is a linear consummation and therefore is less powerful, t-SNE is a non-linear dimension reduction. It has the interesting property that neighboring distances are preserved, therefore it is very suited for 2D and 3D plots of high dimensional data. Autoencoders are autoperforming t-SNE from optimization point of view, therefore it is very interesting to use them for such tasks. Now we've learned a lot about different neural network types. We've learned that the weight matrix, W is essential for learning and training, but we have never learned how to actually come up with a good value set for W. So let's cover this in the next lecture.