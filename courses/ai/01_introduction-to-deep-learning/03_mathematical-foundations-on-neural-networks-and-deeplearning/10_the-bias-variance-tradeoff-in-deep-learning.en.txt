In this video, I will tell you something about Bias-Variance Tradeoff and how you can prevent overfitting of a neural network. So, neural network is a universal function approximator. The first of all, this is really cool. But on the other hand, it means that it tends to fit any dataset ideally and that can be a problem because any dataset contains also lot of noise and does not always reflect the truth of the underlying physical system. Therefore, you shouldn't always fit too strong to your data and try to generalize a bit more. You can divide your dataset into a training and a test dataset. And using the training dataset, you train your neural network and after training, you assess the performance. So, once this is a good value, you use the test dataset and also assess the performance. If the performance is equally good, then you are fine. If the performance drops, you most likely have an overfitting problem. So, here you see data for a binary classification problem in a 2D space. So, this is of course a non-linear separation, but you can draw the line differently. So, you see one line which actually tries to fit every data point perfectly. So, that means on the training set, you will get very, very high accuracy, but once you add additional points which are not fitting this line, then you get an error. So, the best way is to draw a line which makes some errors on the training set, but then performs better on the tests that are on readable data. So, how can we prevent overfitting in neural networks. So, one way is just get more data. That's always the first thing you should try and there's a rule of thumb, you should have 10 times more data than you have weights or parameters in your model. The second thing you can do is regularization. So, regularization is basically a way to penalize high weights. So, you don't have to understand everything here. But the idea is, you have a cost function and on the right hand side, you are basically summing over all the weights and all layers of the neural network. And if you get high weights, then the cost goes up. Therefore, you are penalizing those high weights and neural network tries to fit your data without getting these high weights. And how strong you want to influence this regularization, you can specify the regularization parameter Lambda. Another way of addressing this problem is so-called Early Stopping. So, Early Stopping means, you don't train until you get full convergence of the training set. You stop somehow in the middle. You can, of course, always monitor the training accuracy and validation accuracy or training loss and the validation loss during training. And once the validation loss gets up and the training loss didn't get down, you can just stop with training and you also obtain a more general model. So, then there's Drop Out. In Drop Out, you randomly deactivate neurons in each epoch or in each iteration. So, this causes the neural network to take our path through to graph during training and therefore, also helps in generalization. So, I think now you have quite a good idea how neural networks work and what important parameters are they to tune and therefore, we now can skip to the next section.