1
00:00:00,000 --> 00:00:02,240
Now it's time to do some NLP,

2
00:00:02,240 --> 00:00:07,586
Natural Language Processing, and we will start with the famous word2vec example.

3
00:00:07,586 --> 00:00:10,755
So word2vec is a way to compress

4
00:00:10,755 --> 00:00:14,600
your multidimensional text data into smaller-sized vectors,

5
00:00:14,600 --> 00:00:16,230
and with those vectors,

6
00:00:16,230 --> 00:00:21,915
you can actually do calculations or further attach downstream neural network layers,

7
00:00:21,915 --> 00:00:23,925
for example, for classification.

8
00:00:23,925 --> 00:00:26,005
And in this case, we will just do both.

9
00:00:26,005 --> 00:00:27,915
So we will start with some imports,

10
00:00:27,915 --> 00:00:32,490
and we put here a seat that examples are replicatable.

11
00:00:32,490 --> 00:00:34,290
So let's start with some sentences.

12
00:00:34,290 --> 00:00:38,115
So this is a document which has five sentences,

13
00:00:38,115 --> 00:00:42,968
and four are related to queens and kings and so on and men and women,

14
00:00:42,968 --> 00:00:44,485
and one is unrelated.

15
00:00:44,485 --> 00:00:47,665
We define vocabulary size 50.

16
00:00:47,665 --> 00:00:51,790
That means this system here as it is will support 50 different words.

17
00:00:51,790 --> 00:00:53,140
In a real-life scenario,

18
00:00:53,140 --> 00:00:55,230
of course, you have millions of words.

19
00:00:55,230 --> 00:00:57,990
Later, we do some calculations with the words.

20
00:00:57,990 --> 00:01:02,618
Therefore, we already now get the representation of these words,

21
00:01:02,618 --> 00:01:04,335
and I will show you what I mean by that.

22
00:01:04,335 --> 00:01:07,755
So each word is just a sequence of characters and,

23
00:01:07,755 --> 00:01:10,825
obviously, we cannot work with sequence of characters.

24
00:01:10,825 --> 00:01:14,410
Therefore, we will convert each word into an integer number,

25
00:01:14,410 --> 00:01:16,155
and this integer number is unique,

26
00:01:16,155 --> 00:01:19,340
as long as we don't exceed the vocabulary size.

27
00:01:19,340 --> 00:01:20,855
Note that the function is called one-hot,

28
00:01:20,855 --> 00:01:23,245
but in my opinion, that's not correct.

29
00:01:23,245 --> 00:01:24,634
It's not the one-hot encoding.

30
00:01:24,634 --> 00:01:27,180
It's basically just the transformation from

31
00:01:27,180 --> 00:01:30,590
a list of the words into a list of integer values.

32
00:01:30,590 --> 00:01:32,580
As you can see here, I'm correct.

33
00:01:32,580 --> 00:01:40,025
So, for example, king is number 23 and queen is number 26 or man is number 22, and so on.

34
00:01:40,025 --> 00:01:42,425
So now we are actually one-hot encoding this stuff.

35
00:01:42,425 --> 00:01:45,615
So this is now a single dimension with multiple values.

36
00:01:45,615 --> 00:01:49,425
And one-hot encoding returns multiple dimensions,

37
00:01:49,425 --> 00:01:52,550
and only one of those dimensions has a one.

38
00:01:52,550 --> 00:01:53,790
The rest is zero.

39
00:01:53,790 --> 00:01:58,580
That means we are getting sparse vector representations.

40
00:01:58,580 --> 00:02:00,840
So that's actually what I'm talking about.

41
00:02:00,840 --> 00:02:03,610
So you see here for each word,

42
00:02:03,610 --> 00:02:05,840
we get a sparse vector.

43
00:02:05,840 --> 00:02:10,195
And if the particular word is present at that position we have one,

44
00:02:10,195 --> 00:02:12,040
otherwise we have zero.

45
00:02:12,040 --> 00:02:15,920
So now we have done this for our example words,

46
00:02:15,920 --> 00:02:18,000
but now let's do it for a complete document.

47
00:02:18,000 --> 00:02:20,970
So as you can see here, we have some words again.

48
00:02:20,970 --> 00:02:23,470
So those are the sentences.

49
00:02:23,470 --> 00:02:26,295
Each element in this multidimension array is

50
00:02:26,295 --> 00:02:29,395
a sentence and each integer is representing a word.

51
00:02:29,395 --> 00:02:34,145
It's just an index which tells us at what position we should insert a particular word.

52
00:02:34,145 --> 00:02:44,930
So 23, 42 and 22 are actually "king is man." So king is 23.

53
00:02:44,930 --> 00:02:49,240
We see here, again, king,

54
00:02:49,240 --> 00:02:52,720
a second word in the second sentence.

55
00:02:52,720 --> 00:02:55,600
So here, we are actually padding that means,

56
00:02:55,600 --> 00:02:57,110
if the sentence is not long enough,

57
00:02:57,110 --> 00:02:59,695
we are just filling it with zeros.

58
00:02:59,695 --> 00:03:01,300
This function is very,

59
00:03:01,300 --> 00:03:04,835
very important and may be a bit complicated to understand,

60
00:03:04,835 --> 00:03:06,720
but what we are doing is,

61
00:03:06,720 --> 00:03:09,070
we're now creating tuples.

62
00:03:09,070 --> 00:03:14,130
And we are creating tuples of a word and related words.

63
00:03:14,130 --> 00:03:16,420
So in a sentence, for example,

64
00:03:16,420 --> 00:03:20,020
"king is a man," if you look at the word king,

65
00:03:20,020 --> 00:03:22,565
then we are creating tuples,

66
00:03:22,565 --> 00:03:25,795
where "king" is the key and "is" is the value.

67
00:03:25,795 --> 00:03:28,840
And "king" is the key and "man" is the value.

68
00:03:28,840 --> 00:03:31,260
That means that's the embedding.

69
00:03:31,260 --> 00:03:34,355
That means the neighbors of the word.

70
00:03:34,355 --> 00:03:37,820
So we are creating a list of tuples where,

71
00:03:37,820 --> 00:03:42,800
for each word, we also are assigning the neighbors of the word.

72
00:03:42,800 --> 00:03:46,200
We're doing this, in this case, for two neighbors,

73
00:03:46,200 --> 00:03:50,665
for two preceding neighbors and two succeeding neighbors.

74
00:03:50,665 --> 00:03:52,640
Don't lose too much time with that.

75
00:03:52,640 --> 00:03:53,935
That's the follow-up here.

76
00:03:53,935 --> 00:03:55,388
Just look at recite.

77
00:03:55,388 --> 00:04:00,000
So recite has dimension 38 by two.

78
00:04:00,000 --> 00:04:04,815
That means you have 38 word pairs and the pairs,

79
00:04:04,815 --> 00:04:07,535
as the name implies, have dimension two.

80
00:04:07,535 --> 00:04:15,420
So, again, we have now two dimensions with integer keys.

81
00:04:15,420 --> 00:04:18,405
And in order to make a neural network perform, we, again,

82
00:04:18,405 --> 00:04:21,495
have to encode it as one-hot encoded vectors.

83
00:04:21,495 --> 00:04:25,145
We first do it for the first dimension,

84
00:04:25,145 --> 00:04:29,990
which basically resembles in the input extra neural network,

85
00:04:29,990 --> 00:04:33,300
and then we do it for the second dimension,

86
00:04:33,300 --> 00:04:34,760
which is our target.

87
00:04:34,760 --> 00:04:40,160
So that's what we want to predict and that's why or the output of the neural network.

88
00:04:40,160 --> 00:04:42,300
So here you see that, again,

89
00:04:42,300 --> 00:04:44,535
we have 38 dimensions because,

90
00:04:44,535 --> 00:04:47,985
in the previous example, we had 38 pairs,

91
00:04:47,985 --> 00:04:51,135
but now we have 50 dimensions because now

92
00:04:51,135 --> 00:04:56,885
the one-hot encoded vector representation is the sparse vector of size 50.

93
00:04:56,885 --> 00:04:59,185
Remember, only one of those elements is one.

94
00:04:59,185 --> 00:05:03,240
And that's the position which actually reflects the word.

95
00:05:03,240 --> 00:05:06,300
So if you start with a dense input layer which has

96
00:05:06,300 --> 00:05:11,740
50 internal neurons and also expects dimensionality of 50,

97
00:05:11,740 --> 00:05:17,240
actually, that perfectly matches our one-hot encoded vectors of size 50.

98
00:05:17,240 --> 00:05:19,865
And we use relu as activation.

99
00:05:19,865 --> 00:05:21,950
So now it gets interesting.

100
00:05:21,950 --> 00:05:23,475
Now we're increasing the bottleneck.

101
00:05:23,475 --> 00:05:25,680
Remember bottlenecks. We have an auto encoder.

102
00:05:25,680 --> 00:05:28,665
So this is an auto encoder of what we are building,

103
00:05:28,665 --> 00:05:32,530
and the bottleneck has only two dimensions.

104
00:05:32,530 --> 00:05:35,485
And, again, activation function is relu.

105
00:05:35,485 --> 00:05:37,950
And in an auto encoder, of course,

106
00:05:37,950 --> 00:05:40,915
we have to map again to the same dimensionality.

107
00:05:40,915 --> 00:05:43,070
So because input was of dimension 50,

108
00:05:43,070 --> 00:05:45,765
also the output has to be of dimension 50.

109
00:05:45,765 --> 00:05:50,405
So we're mapping sparse vectors to sparse vectors.

110
00:05:50,405 --> 00:05:53,838
That means we are mapping the words,

111
00:05:53,838 --> 00:05:58,965
for example, a word in the center to words in the neighborhood.

112
00:05:58,965 --> 00:06:00,970
And while we're doing this,

113
00:06:00,970 --> 00:06:04,150
we are training the bottleneck layer,

114
00:06:04,150 --> 00:06:06,660
and that's our low-dimensional representation.

115
00:06:06,660 --> 00:06:09,940
And that's actually what word2vec does.

116
00:06:09,940 --> 00:06:16,067
It creates a low-dimensional representation of our high-dimensional word embedding,

117
00:06:16,067 --> 00:06:19,150
which then basically is a low-dimensional word embedding.

118
00:06:19,150 --> 00:06:21,415
We compile, set optimizer,

119
00:06:21,415 --> 00:06:24,730
and loss function, and then we call fit.

120
00:06:24,730 --> 00:06:33,555
So that starts training on the one-hot encoded labels X and the one-hot encoded target Y.

121
00:06:33,555 --> 00:06:34,810
So for this Mickey Mouse dataset,

122
00:06:34,810 --> 00:06:36,846
this goes really fast,

123
00:06:36,846 --> 00:06:39,790
and we are quite happy with our loss and accuracy.

124
00:06:39,790 --> 00:06:41,880
So now it gets even more interesting.

125
00:06:41,880 --> 00:06:45,340
Now we build a new neural network but we are using

126
00:06:45,340 --> 00:06:49,790
two of the three already trained layers of the previous neural network.

127
00:06:49,790 --> 00:06:52,480
So we start with a sequential model again,

128
00:06:52,480 --> 00:06:55,270
and we add the already trained input layer,

129
00:06:55,270 --> 00:06:57,980
and we add the already trained bottleneck layer.

130
00:06:57,980 --> 00:07:00,710
So if we now compile this neural network,

131
00:07:00,710 --> 00:07:06,220
the input still is 50 dimensional but the output now is only two dimensional.

132
00:07:06,220 --> 00:07:08,748
So we take all five words of the beginning,

133
00:07:08,748 --> 00:07:13,525
and let's see what the two-dimensional representations of those words are looking like.

134
00:07:13,525 --> 00:07:16,535
So we see this is an array of two-dimensional vectors,

135
00:07:16,535 --> 00:07:20,540
and in theory, we can now plot those words into 2D space.

136
00:07:20,540 --> 00:07:23,585
But we can also calculate with those vectors.

137
00:07:23,585 --> 00:07:28,390
So we now take the word king, we subtract man,

138
00:07:28,390 --> 00:07:35,915
we add woman, and this should resemble nearly the same vector as the word "queen."

139
00:07:35,915 --> 00:07:41,310
That means if we now subtract "queen" from it we should get a relatively low value.

140
00:07:41,310 --> 00:07:47,825
Since this is only a try example with very little data and training takes a lot of time,

141
00:07:47,825 --> 00:07:51,005
I'm using some online demo of this system.

142
00:07:51,005 --> 00:07:54,205
So here, we actually take woman,

143
00:07:54,205 --> 00:07:58,700
and here we say king minus man.

144
00:07:58,700 --> 00:08:01,955
So we get somehow the meaning of king without a gender.

145
00:08:01,955 --> 00:08:05,755
And if we add this meaning without a gender to a woman,

146
00:08:05,755 --> 00:08:07,245
we should get "queen."

147
00:08:07,245 --> 00:08:11,275
Let's see what happens. Yes, see. We are pretty close.

148
00:08:11,275 --> 00:08:12,560
So that's actually really cool.

149
00:08:12,560 --> 00:08:17,100
We can now calculate with words in a low-dimensional vector space.