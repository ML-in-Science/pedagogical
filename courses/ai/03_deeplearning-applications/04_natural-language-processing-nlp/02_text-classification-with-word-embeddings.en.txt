So, if this is a bit complicated then luckily Carris supports all this functionality in one single layer which is quite Embedding. So, embedding takes an input of a certain dimension and actually creates a lot of measure of presentation for us. So now, we have a vocabulary of a thousand possible words and we have three sentences and each sentence contains exactly five words. So, no need for padding in this simple example. And this should give us a three by five matrix which is actually correct, because we have three sentences and five words per sentence. And instead of words we have integer representations of the words so that is actually correct here. So, the output is of shape three by five by three, so it's a 3D Tensor. It can be three sentences and five words per sentence. But now each word is not represented as a single dimensional integer scaler but it is represented as a vector of size three because it defined that if you compress this high dimensional space into a lower dimensional space of size three, of dimensional relative three. So, let's actually inspect this Tensor. So the first element here is a sentence and it's a list of words and each word has three dimensions. So now, we've understood how the embedding layer works and let's use this layer to create a very simple document classifier. We you want to classify the sentiment of a text whether it is positive or negative. So this document contains a couple of sentences. They are relatively simple so we have five negative and five positive sentences and we assign a class label to each sentence. One stands for positive and zero stands for negative sentiment. And again, we use the so-called one hot and quarter from Cara's text which in my opinion again is not one hot and quarter. It just gives us the integer presentation for each word. So for example 29, this work and you see work in two sentences and now we are padding because our longest sentence has four words we have padding to four. That means the short sentences are just padded with creating zeroes. So now, let's create an embedding in this case we will reduce the dimensionality to dimensionality of eight. The max length is four and then we have flatten because we don't want to work with 3D Tensor. And then we will feed in the result into a dense layer which has only one dimension because we are predicting a binary target zero or one and therefore we also use the sigmoid activation function. Use "Adam" as optimize the binary cross entropy as less function. So note that the embedding layer has already 400 parameters. So now, it's time to call "Fit" and note here we have the padded one hot encoded sentences as input and the class labels as target or as output to newer network. We are training for 50 epochs and immediately we measure accuracy. So, we are at 80 percent which is not bad but we can improve this of course. And now, let's create a little overview of how good we are doing per sentence. So, we call predict that gives us the output of the sigmoid activation and in the second column we just use the actual labor. And as you can see in most of the cases the prediction is correct. So, whatever we see a one the value is bigger than zero dot five and wherever the value is zero It's less than zero dot five. So you see that actually it works. Whenever the class labels is one, you see an activation bigger then zero dot five. And in most of the cases when the class labels is zero you see an activation which is lower than zero dot five. So, this is only little example but that's actually the way how document classification is done in real worlds scenarios.