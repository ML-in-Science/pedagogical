1
00:00:00,000 --> 00:00:03,905
Welcome back. Last time,

2
00:00:03,905 --> 00:00:09,680
we have discussed a very important topic of batch size.

3
00:00:09,680 --> 00:00:14,660
Today, we're going to continue on this and we are going

4
00:00:14,660 --> 00:00:21,340
further and discussing one another very important parameter,

5
00:00:21,340 --> 00:00:28,710
which we need to set, is the number of timesteps.

6
00:00:28,710 --> 00:00:36,460
But first of all, we're starting by importing the required packages.

7
00:00:36,460 --> 00:00:42,623
So, we are importing standard Numpy.

8
00:00:42,623 --> 00:00:45,835
We are importing Matplotlib.

9
00:00:45,835 --> 00:00:52,090
We import pandas, because by dealing with a pandas data frames,

10
00:00:52,090 --> 00:01:02,730
and we already here import Keras pre-processing sequence package.

11
00:01:02,730 --> 00:01:11,890
In the last session, we have said that we are going to select batch size of 64.

12
00:01:11,890 --> 00:01:14,235
Now, we will set to

13
00:01:14,235 --> 00:01:22,720
another important parameter which is number of epochs and number of timesteps.

14
00:01:22,720 --> 00:01:25,000
What is number of epochs?

15
00:01:25,000 --> 00:01:29,752
An epoch is a training ground.

16
00:01:29,752 --> 00:01:35,800
The more you train your neural network,

17
00:01:35,800 --> 00:01:39,565
the better it should get.

18
00:01:39,565 --> 00:01:46,855
Here, I have chosen Honda 20.

19
00:01:46,855 --> 00:01:53,562
In the later sessions and also in programming assignment,

20
00:01:53,562 --> 00:02:04,330
we are going to see how the number of epochs impacts the prediction quality.

21
00:02:04,330 --> 00:02:10,945
Here, the second parameter is number of timesteps.

22
00:02:10,945 --> 00:02:18,760
What is the meaning of this parameter, especially for LSTM?

23
00:02:18,760 --> 00:02:23,895
This is a very important setting.

24
00:02:23,895 --> 00:02:27,040
How do we predict?

25
00:02:27,040 --> 00:02:36,730
To understand this, imagine a rolling window.

26
00:02:36,730 --> 00:02:43,945
This window is divided in two parts with a red vertical line.

27
00:02:43,945 --> 00:02:49,270
This red vertical line is exactly in the middle of

28
00:02:49,270 --> 00:02:54,020
this window and this is the point of now.

29
00:02:54,020 --> 00:03:02,603
This is the point where we are at the moment.

30
00:03:02,603 --> 00:03:08,185
The 10 timesteps to the left is the past,

31
00:03:08,185 --> 00:03:14,103
and the 10 timesteps to the right is the future.

32
00:03:14,103 --> 00:03:20,200
So, the LSTM network is learning from the data,

33
00:03:20,200 --> 00:03:24,380
from the 10 timesteps which are allocated to the left,

34
00:03:24,380 --> 00:03:29,360
to predict the data which is

35
00:03:29,360 --> 00:03:35,378
located to the right of this red vertical line.

36
00:03:35,378 --> 00:03:38,890
And this sliding window,

37
00:03:38,890 --> 00:03:46,830
every time the LSTM has learned from 10 timesteps and

38
00:03:46,830 --> 00:03:56,058
has attempted to predict the 10 timesteps in the future,

39
00:03:56,058 --> 00:04:03,295
the whole sliding window slides one timestep to the right,

40
00:04:03,295 --> 00:04:06,620
and again, the whole procedure starts.

41
00:04:06,620 --> 00:04:15,545
It takes 10 timesteps to the left from now, from the past,

42
00:04:15,545 --> 00:04:17,950
it learns from it,

43
00:04:17,950 --> 00:04:20,790
it learns from this data,

44
00:04:20,790 --> 00:04:27,925
and then tries to predict what is going to be in the future,

45
00:04:27,925 --> 00:04:31,135
in the 10 timesteps in the future.

46
00:04:31,135 --> 00:04:33,670
And after it's completed,

47
00:04:33,670 --> 00:04:41,289
you move the sliding window again to the right one timesteps.

48
00:04:41,289 --> 00:04:45,720
This is called stright, this move.

49
00:04:45,720 --> 00:04:49,085
Normally, it's one timestep.

50
00:04:49,085 --> 00:04:54,465
You can control it but the default setting is always one.

51
00:04:54,465 --> 00:04:59,260
And now, we're selecting 10 timesteps.

52
00:04:59,260 --> 00:05:01,490
In this special data set,

53
00:05:01,490 --> 00:05:06,835
one timestep is meaning one day.

54
00:05:06,835 --> 00:05:13,680
So everyday, the crude oil prices are measured,

55
00:05:13,680 --> 00:05:19,675
and we have 10 timesteps in the past,

56
00:05:19,675 --> 00:05:27,095
where we learn how the prices were to predict 10 days,

57
00:05:27,095 --> 00:05:29,546
10 timesteps in the future.

58
00:05:29,546 --> 00:05:35,920
It's nice, actually, if you look at it as a trader, for example,

59
00:05:35,920 --> 00:05:41,245
and you have this neural network,

60
00:05:41,245 --> 00:05:45,720
you can actually predict,

61
00:05:45,720 --> 00:05:51,915
if you have the history data and you have trained your neural network,

62
00:05:51,915 --> 00:05:59,445
you have now 10 days which are already recorded in the past,

63
00:05:59,445 --> 00:06:06,427
you can predict what is going to be with the prices 10 days in the future.

64
00:06:06,427 --> 00:06:08,470
And this is nice.

65
00:06:08,470 --> 00:06:12,235
So in the next session,

66
00:06:12,235 --> 00:06:20,665
we're going to discuss more in detail how we set

67
00:06:20,665 --> 00:06:33,890
the training set size with regard to the batch size.

68
00:06:33,890 --> 00:06:43,150
This is an important thing because we are working with a stateful LSTM.

69
00:06:43,150 --> 00:06:50,430
And therefore, the training set size

70
00:06:50,430 --> 00:06:57,485
must be entirely dividable by the batch size.

71
00:06:57,485 --> 00:07:06,830
Why so? This is because the batches are actually trainings units,

72
00:07:06,830 --> 00:07:16,765
so the neural network takes one batch of 64 observations.

73
00:07:16,765 --> 00:07:19,905
It will learn from it.

74
00:07:19,905 --> 00:07:26,685
It will update the parameters,

75
00:07:26,685 --> 00:07:34,940
as we said in a cell state and the hidden state,

76
00:07:34,940 --> 00:07:43,195
and these parameters will be transferred as initial state to the next batch.

77
00:07:43,195 --> 00:07:45,985
And the whole procedure starts again.

78
00:07:45,985 --> 00:07:52,835
So the trainings set consists of these batches.

79
00:07:52,835 --> 00:07:56,635
And therefore, you must set

80
00:07:56,635 --> 00:08:04,379
the training set size so you can divide by the batch size,

81
00:08:04,379 --> 00:08:09,695
meaning, major of such division must be zero.

82
00:08:09,695 --> 00:08:20,160
And we're going to define a method which provides us

83
00:08:20,160 --> 00:08:25,204
with the exact test set size

84
00:08:25,204 --> 00:08:31,005
given the data set and the batch size.

85
00:08:31,005 --> 00:08:36,630
This will be intensively discussed in the next session.

86
00:08:36,630 --> 00:08:40,760
Until then, enjoy and stay tuned. Bye.