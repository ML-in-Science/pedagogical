In this module, we will cover Automatic Differentiation, one of the key features of TensorFlow. May be from high school if you remember what differentiation, and the chain rule of differentiation is. If not just, please have a look at the next two slides for a little refresher. So, if you take the first derivative of cosine of x, it turns out that this is just minus sign of x. Let's create a little plot to illustrate this. So, the green line is cosine of x and the blue line is the first derivative of cosine of x. As you might remember, the first derivative tells us something about a slope of the function. So here at this point, where the slope is zero, the value of the first derivative is zero as well. At the this deepest position, the derivative becomes minus one. Again, the cosine function reaches zero gradient. And this is reflected by the first derivative as well. Finally, when the cosine function reached its maximum ascending gradient, the first derivative becomes one. So now, let TensorFlow take care of computing the first derivative of the cosine function. We start with a value x and apply TensorFlow's cosine operation on it. Then we tell TensorFlow to minimize overbuy. In order to achieve this, TensorFlow must compute the first derivative of the cosine function. Note that until now, no competitions have happened. Only the execution graph has been created. Let's have a look at this graph in TensorBoard now. So, this is the graph of our example, which contains the created operation. Within this section, the gradient of the cosine function is computed, which is actually nothing else than the first derivative of the function. As you can see, this is reflected by the appropriate operations. It turns out that for every atomic mathematical operation, it is guaranteed that there exists a derivative operation. And with a chain rule of differentiation, we are guaranteed to obtain a solution for the first derivative, no matter how complex the functions are getting. But this is beyond the scope of this course. Just remember, every operator which is available within TensorFlow has the richest and appropriate created function. Here in this case, an operator called, ZeroOut, is RegisterGradient function called zero_out_grad. This way, TensorFlow is capable of automatically compute the derivative of any function defined in a TensorFlow graph. So, we only need to come up with a creative idea of a model, and TensorFlow does the rest. Isn't that great?